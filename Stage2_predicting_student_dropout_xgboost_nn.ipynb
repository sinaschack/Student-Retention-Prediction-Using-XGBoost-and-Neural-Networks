{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2"
      ],
      "metadata": {
        "id": "T8eIfRreYUoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File URL\n",
        "file_url_2 = \"https://drive.google.com/uc?id=1vy1JFQZva3lhMJQV69C43AB1NTM4W-DZ\"\n",
        "\n",
        "# Loads the CSV file from Google Drive into a pandas DataFrame\n",
        "stage2_data = pd.read_csv(f\"https://drive.google.com/uc?export=download&id={file_url_2.split('=')[-1]}\")\n",
        "\n",
        "# View the first few rows\n",
        "stage2_data.head()"
      ],
      "metadata": {
        "id": "4Dg28A4C-Noo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1IFpj7wUZ6x"
      },
      "outputs": [],
      "source": [
        "# Check data types and missing values\n",
        "stage2_data.info()\n",
        "\n",
        "# Quick summary of data\n",
        "stage2_data.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLEaPzLudwC1"
      },
      "outputs": [],
      "source": [
        "# Drop 'LearnerCode' – it's just an ID, not predictive\n",
        "stage2_data.drop(columns=['LearnerCode'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQnJCcaRdwC1"
      },
      "outputs": [],
      "source": [
        "# Convert 'DateofBirth' to datetime\n",
        "stage2_data['DateofBirth'] = pd.to_datetime(stage2_data['DateofBirth'], errors='coerce', dayfirst=True)\n",
        "\n",
        "# Compute age assuming data collected in 2016\n",
        "stage2_data['Age'] = 2016 - stage2_data['DateofBirth'].dt.year\n",
        "\n",
        "# Why 2016?\n",
        "# A student born in 1998 is listed under Foundation, which is typically for students around 18 years old.\n",
        "# That suggests this record was collected around 2016 (1998 + 18).\n",
        "\n",
        "# Drop the original DateofBirth column\n",
        "stage2_data.drop(columns=['DateofBirth'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hem2xd4FdwC1"
      },
      "outputs": [],
      "source": [
        "# List all columns in the DataFrame\n",
        "for col in stage2_data.columns:\n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JN7ECdfdwC2"
      },
      "outputs": [],
      "source": [
        "# Identify columns with >200 unique values\n",
        "high_cardinality_cols = [col for col in stage2_data.columns if stage2_data[col].nunique() > 200]\n",
        "\n",
        "# Print columns that will be dropped\n",
        "print(\"Dropped columns due to high cardinality (>200 unique values):\")\n",
        "print(high_cardinality_cols)\n",
        "\n",
        "# Drop those columns\n",
        "stage2_data.drop(columns=high_cardinality_cols, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdaHm5UndwC3"
      },
      "outputs": [],
      "source": [
        "# Save original column names\n",
        "original_columns = set(stage2_data.columns)\n",
        "\n",
        "# Drop columns where more than 50% of the data is missing\n",
        "threshold = len(stage2_data) * 0.5\n",
        "stage2_data.dropna(thresh=threshold, axis=1, inplace=True)\n",
        "\n",
        "# Save new column names\n",
        "remaining_columns = set(stage2_data.columns)\n",
        "\n",
        "# Find which columns were dropped\n",
        "dropped_columns = original_columns - remaining_columns\n",
        "print(\"Dropped columns due to >50% missing values:\", dropped_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGd1g0wddwC3"
      },
      "outputs": [],
      "source": [
        "#from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Example: assuming 'stage1_data' is your DataFrame\n",
        "missing_percent = stage2_data.isnull().mean() * 100\n",
        "initial_row_count = len(stage2_data)\n",
        "\n",
        "# Dictionary to store dropped info\n",
        "dropped_info = {}\n",
        "\n",
        "# Drop rows if missing value is <2% in that column\n",
        "for col in stage2_data.columns:\n",
        "    if 0 < missing_percent[col] < 2:\n",
        "        missing_rows = stage2_data[col].isnull().sum()\n",
        "        dropped_info[col] = {\n",
        "            'rows_dropped': missing_rows,\n",
        "            'percent_of_total_rows': (missing_rows / initial_row_count) * 100\n",
        "        }\n",
        "        stage1_data = stage2_data[~stage2_data[col].isnull()]\n",
        "\n",
        "# Print the info\n",
        "if dropped_info:\n",
        "    print(\"Columns with dropped rows (missing < 2%):\")\n",
        "    for col, info in dropped_info.items():\n",
        "        print(f\"- {col}: {info['rows_dropped']} rows dropped \"\n",
        "              f\"({info['percent_of_total_rows']:.2f}% of total)\")\n",
        "else:\n",
        "    print(\"No columns had missing values <2%, so no rows were dropped.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = stage2_data.select_dtypes(include=np.number).columns\n",
        "if len(numeric_cols) > 0:\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    stage2_data[numeric_cols] = imputer.fit_transform(stage2_data[numeric_cols])\n",
        "    print(f\"Imputed missing values in numeric columns: {list(numeric_cols)}\")\n",
        "else:\n",
        "    print(\"No numeric columns found for imputation, skipping this step.\")"
      ],
      "metadata": {
        "id": "Awwzs_hTXJVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTbQy4yklBwq"
      },
      "outputs": [],
      "source": [
        "# Encode the target first, before it gets one-hot encoded\n",
        "stage2_data['CompletedCourse'] = stage2_data['CompletedCourse'].map({'Yes': 1, 'No': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riOraxrIlBwr"
      },
      "outputs": [],
      "source": [
        "# One-hot encode the other categorical features (excluding the target)\n",
        "categorical_cols = stage2_data.select_dtypes(include=['object', 'category']).columns\n",
        "categorical_cols = categorical_cols.drop('CompletedCourse', errors='ignore')\n",
        "\n",
        "stage2_data = pd.get_dummies(stage2_data, columns=categorical_cols, drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhvbU-2-imUz"
      },
      "outputs": [],
      "source": [
        "# Count how many samples in each class\n",
        "print(stage2_data['CompletedCourse'].value_counts())\n",
        "\n",
        "# Same in %\n",
        "print(stage2_data['CompletedCourse'].value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBeO375DimU1"
      },
      "outputs": [],
      "source": [
        "# Plot histogram to check distribution\n",
        "stage2_data['CompletedCourse'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Distribution of CompletedCourse\")\n",
        "plt.xlabel(\"Completed (1 = Yes, 0 = No)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y95EEr54imU2"
      },
      "outputs": [],
      "source": [
        "# Split data into X (features) and y (target)\n",
        "X = stage2_data.drop(columns=['CompletedCourse'])\n",
        "y = stage2_data['CompletedCourse']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66Z9aKbfimU2"
      },
      "outputs": [],
      "source": [
        "# Split into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Have binary labels 0 and 1 -> can use scale_pos_weight as:\n",
        "num_neg = sum(y_train == 0)\n",
        "num_pos = sum(y_train == 1)\n",
        "scale_pos_weight = num_neg / num_pos"
      ],
      "metadata": {
        "id": "ZLUauHpTj7Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_xgb = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss')\n",
        "\n",
        "model_xgb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Ffafgfg-nJN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(3, 1, figsize=(15, 18))\n",
        "\n",
        "# Gain (average gain in accuracy) - top plot\n",
        "# How much a feature improves the model’s accuracy when it’s used for splitting.\n",
        "xgb.plot_importance(model_xgb, importance_type='gain', ax=axes[0], max_num_features=10, title='Feature Importance (Gain)')\n",
        "\n",
        "# Weight (frequency) - middle plot\n",
        "# Definition: How many times a feature is used to split nodes in all trees.\n",
        "xgb.plot_importance(model_xgb, importance_type='weight', ax=axes[1], max_num_features=10, title='Feature Importance (Weight)')\n",
        "\n",
        "# Cover (average coverage) - bottom plot\n",
        "# How many data points are affected by splits on that feature.\n",
        "xgb.plot_importance(model_xgb, importance_type='cover', ax=axes[2], max_num_features=10, title='Feature Importance (Cover)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QnjhFdyhmeJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get importance dictionaries\n",
        "gain_importance = model_xgb.get_booster().get_score(importance_type='gain')\n",
        "weight_importance = model_xgb.get_booster().get_score(importance_type='weight')\n",
        "cover_importance = model_xgb.get_booster().get_score(importance_type='cover')\n",
        "\n",
        "# Helper function to create sorted DataFrame for a given importance type\n",
        "def create_importance_df(importance_dict, top_n=10):\n",
        "    df = pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Value'])\n",
        "    df_sorted = df.sort_values(by='Value', ascending=False).head(top_n).reset_index(drop=True)\n",
        "    return df_sorted\n",
        "\n",
        "# Create tables\n",
        "gain_df = create_importance_df(gain_importance)\n",
        "weight_df = create_importance_df(weight_importance)\n",
        "cover_df = create_importance_df(cover_importance)\n",
        "\n",
        "# Display the tables\n",
        "print(\"Top 10 Features by Gain:\")\n",
        "print(gain_df, \"\\n\")\n",
        "\n",
        "print(\"Top 10 Features by Weight:\")\n",
        "print(weight_df, \"\\n\")\n",
        "\n",
        "print(\"Top 10 Features by Cover:\")\n",
        "print(cover_df)"
      ],
      "metadata": {
        "id": "Bsr4r-w2meJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_xgb = model_xgb.predict(X_test)\n",
        "y_pred_proba_xgb = model_xgb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Metrics\n",
        "cm = confusion_matrix(y_test, y_pred_xgb)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
        "print(f\"AUC Score: {roc_auc_score(y_test, y_pred_proba_xgb):.4f}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_xgb)\n",
        "auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nQxZKUKqnfhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets (80% train, 20% validation)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "rKZ-N5iiQBmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Optuna objective function\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "        'scale_pos_weight': scale_pos_weight,\n",
        "        'eval_metric': 'logloss',\n",
        "        'random_state': 42,\n",
        "        'use_label_encoder': False,\n",
        "        'verbosity': 0\n",
        "    }\n",
        "\n",
        "    model = XGBClassifier(**params)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    recall_scores = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        recall_0 = recall_score(y_val, preds, pos_label=0)\n",
        "        recall_scores.append(recall_0)\n",
        "\n",
        "    return np.mean(recall_scores)\n",
        "\n",
        "# Run the study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"Best Hyperparameters from Optuna tuning:\")\n",
        "print(study.best_trial.params)\n",
        "print(f\"Best Mean Recall for Class 0 (Dropouts): {study.best_value:.4f}\")\n",
        "\n",
        "# Use best params from study\n",
        "best_params = study.best_trial.params\n",
        "best_params.update({\n",
        "    'scale_pos_weight': scale_pos_weight,\n",
        "    'eval_metric': 'logloss',\n",
        "    'random_state': 42,\n",
        "    'use_label_encoder': False,\n",
        "    'verbosity': 0\n",
        "})\n",
        "\n",
        "model_xgb_optimised = XGBClassifier(**best_params)\n",
        "\n",
        "# Train on full training data\n",
        "model_xgb_optimised.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate on test data\n",
        "y_pred_xgb = model_xgb_optimised.predict(X_test)\n",
        "y_pred_proba_xgb = model_xgb_optimised.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Metrics and confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_xgb)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
        "print(f\"AUC Score: {roc_auc_score(y_test, y_pred_proba_xgb):.4f}\")\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6ks4ZGNqP1_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_xgb)\n",
        "auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SvpNJZANq0Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(3, 1, figsize=(15, 18))\n",
        "\n",
        "# Gain (average gain in accuracy) - top plot\n",
        "# How much a feature improves the model’s accuracy when it’s used for splitting.\n",
        "xgb.plot_importance(model_xgb, importance_type='gain', ax=axes[0], max_num_features=10, title='Feature Importance (Gain)')\n",
        "\n",
        "# Weight (frequency) - middle plot\n",
        "# Definition: How many times a feature is used to split nodes in all trees.\n",
        "xgb.plot_importance(model_xgb, importance_type='weight', ax=axes[1], max_num_features=10, title='Feature Importance (Weight)')\n",
        "\n",
        "# Cover (average coverage) - bottom plot\n",
        "# How many data points are affected by splits on that feature.\n",
        "xgb.plot_importance(model_xgb, importance_type='cover', ax=axes[2], max_num_features=10, title='Feature Importance (Cover)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "trof7aA1q8Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get importance dictionaries\n",
        "gain_importance = model_xgb.get_booster().get_score(importance_type='gain')\n",
        "weight_importance = model_xgb.get_booster().get_score(importance_type='weight')\n",
        "cover_importance = model_xgb.get_booster().get_score(importance_type='cover')\n",
        "\n",
        "# Helper function to create sorted DataFrame for a given importance type\n",
        "def create_importance_df(importance_dict, top_n=10):\n",
        "    df = pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Value'])\n",
        "    df_sorted = df.sort_values(by='Value', ascending=False).head(top_n).reset_index(drop=True)\n",
        "    return df_sorted\n",
        "\n",
        "# Create tables\n",
        "gain_df = create_importance_df(gain_importance)\n",
        "weight_df = create_importance_df(weight_importance)\n",
        "cover_df = create_importance_df(cover_importance)\n",
        "\n",
        "# Display the tables\n",
        "print(\"Top 10 Features by Gain:\")\n",
        "print(gain_df, \"\\n\")\n",
        "\n",
        "print(\"Top 10 Features by Weight:\")\n",
        "print(weight_df, \"\\n\")\n",
        "\n",
        "print(\"Top 10 Features by Cover:\")\n",
        "print(cover_df)"
      ],
      "metadata": {
        "id": "foWT2Ph1rKEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics from both models (default and tuned)\n",
        "metrics_data = {\n",
        "    'Metric': [\n",
        "        'Accuracy',\n",
        "        'AUC Score',\n",
        "        'Precision (Class 0)',\n",
        "        'Precision (Class 1)',\n",
        "        'Recall (Class 0)',\n",
        "        'Recall (Class 1)',\n",
        "        'F1-Score (Class 0)',\n",
        "        'F1-Score (Class 1)',\n",
        "        'True Negatives (TN)',\n",
        "        'False Positives (FP)',\n",
        "        'False Negatives (FN)',\n",
        "        'True Positives (TP)'\n",
        "    ],\n",
        "    'Default Model': [\n",
        "        0.8659,\n",
        "        0.9012,\n",
        "        0.54,\n",
        "        0.96,\n",
        "        0.78,\n",
        "        0.88,\n",
        "        0.64,\n",
        "        0.92,\n",
        "        587,\n",
        "        164,\n",
        "        508,\n",
        "        3753\n",
        "    ],\n",
        "    'Tuned Model': [\n",
        "        0.8651,\n",
        "        0.8987,\n",
        "        0.53,\n",
        "        0.96,\n",
        "        0.78,\n",
        "        0.88,\n",
        "        0.63,\n",
        "        0.92,\n",
        "        585,\n",
        "        166,\n",
        "        510,\n",
        "        3751\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "# Calculate percentage change from default to tuned\n",
        "def pct_change(default, tuned):\n",
        "    return np.where(default != 0, (tuned - default) / default * 100, np.nan)\n",
        "\n",
        "metrics_df['Percentage Change (%)'] = pct_change(metrics_df['Default Model'], metrics_df['Tuned Model'])\n",
        "\n",
        "# Format display: 4 decimals for models, + sign for percentage changes\n",
        "styled_df = metrics_df.style.format({\n",
        "    'Default Model': '{:.4f}',\n",
        "    'Tuned Model': '{:.4f}',\n",
        "    'Percentage Change (%)': '{:+.2f}%'\n",
        "}).set_caption(\"Model Performance Comparison with Percentage Change\").set_properties(**{'text-align': 'center'})\n",
        "\n",
        "styled_df"
      ],
      "metadata": {
        "id": "8FQ_zNKssZBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Bar plot for percentage changes using your metrics_df DataFrame\n",
        "bars = plt.bar(metrics_df[\"Metric\"], metrics_df[\"Percentage Change (%)\"], color='skyblue')\n",
        "\n",
        "\n",
        "plt.axhline(0, color='gray', linewidth=0.8)\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.ylabel(\"Percentage Change (%)\")\n",
        "plt.title(\"Percentage Change of Metrics: Tuned vs Default XGBoost\")\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.annotate(f'{height:+.2f}%',\n",
        "                 xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                 xytext=(0, 3),\n",
        "                 textcoords=\"offset points\",\n",
        "                 ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "srWwxgxssZBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decided to scale the data for better convergence\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "cuLStz1v0g8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Handles class imbalance\n",
        "# Cares more about underrepresented class by giving it a higher penalty when misclassified\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "lKUQhmVb0kgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'), # 64 neruons are moderate complexity/ relu makes training fast\n",
        "    Dropout(0.3), # Prevents overfitting by randomly dropping 30% of neurons\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2), # Drops 20% to reduce overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "j15q7iFU1DrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "model.compile(\n",
        "    optimizer=Adam(), # handles the imbalanced classes well\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "3H5pnpiw1MQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    class_weight=class_weight_dict,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "qXZ12DpD1MQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_probs = model.predict(X_test_scaled)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "C8vuqJNU16f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Classification Report (Precision, Recall, F1)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# AUC\n",
        "auc = roc_auc_score(y_test, y_pred_probs)\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rGGdadUE1yDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Loss and Accuracy Curves\n",
        "\n",
        "# Extract values from the training history\n",
        "history_dict = history.history\n",
        "\n",
        "train_loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "train_acc = history_dict.get('accuracy')  # Might be 'acc' in older Keras\n",
        "val_acc = history_dict.get('val_accuracy')\n",
        "\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "# Plot loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss, label='Training Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_acc, label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9C3sq2C43GUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model-building function for Keras Tuner\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(X_train_scaled.shape[1],)))\n",
        "\n",
        "    # Tune number of units in first Dense layer\n",
        "    units = hp.Choice('units', values=[32, 64, 128], default=64)\n",
        "    model.add(Dense(units, activation=hp.Choice('activation', ['relu', 'tanh'], default='tanh'))) # Tune activation function to find the best nonlinearity for the data\n",
        "    model.add(Dropout(hp.Float('dropout_1', 0.1, 0.5, step=0.1, default=0.3)))\n",
        "\n",
        "    # Second Dense layer adds capacity if needed (flexibility to increase complexity)\n",
        "    if hp.Boolean('second_layer', default=True):\n",
        "        units2 = hp.Int('units_2', min_value=16, max_value=64, step=16, default=32)\n",
        "        model.add(Dense(units2, activation=hp.Choice('activation_2', ['relu', 'tanh'], default='tanh')))\n",
        "        model.add(Dropout(hp.Float('dropout_2', 0.1, 0.3, step=0.1, default=0.2))) # The dropout ranges (0.1 to 0.5) help address potential overfitting seen in the fluctuating validation loss.\n",
        "        # Additional dropout for the second layer to control overfitting\n",
        "        dropout_2 = hp.Float('dropout_2', 0.1, 0.3, step=0.1, default=0.2)\n",
        "        model.add(Dropout(dropout_2))\n",
        "    # Output layer with sigmoid activation for binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Tune learning rate (1e-4 to 1e-2) to optimize training convergence and stability\n",
        "    lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=lr),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "    # With the use of compiled Keras Sequential models, the functionality can be configured\n",
        "    # with different tunable layers, activations, dropout, and learning rates, which can then lead to training."
      ],
      "metadata": {
        "id": "fh3a7ZX65XZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the best hyperparameters by running multiple model configurations\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy', # maximize validation accuracy\n",
        "    max_trials=10, # 15 was used as a starting point but reduced the number of trials to save time, but fewer trials may limit the search quality\n",
        "    executions_per_trial=1,\n",
        "    directory='hyperparam_tuning',\n",
        "    project_name='nn_imbalanced_data'\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=30, # Number of epochs to train each trial\n",
        "    batch_size=32, # Number of samples per gradient update\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    class_weight=class_weight_dict, # Weights to handle class imbalance during training\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "Ch7RjztB5XZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best model found by the tuner\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_probs = best_model.predict(X_test_scaled).ravel()\n",
        "y_pred = (y_pred_probs > 0.5).astype(int) # Convert probabilities to binary class predictions using 0.5 threshold\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, digits=4)\n",
        "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best tuned model accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "print(f\"AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X0Z-vY2--Kh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Loss and Accuracy Curves\n",
        "\n",
        "# Extract values from the training history\n",
        "history_dict = history.history\n",
        "\n",
        "train_loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "train_acc = history_dict.get('accuracy')  # Might be 'acc' in older Keras\n",
        "val_acc = history_dict.get('val_accuracy')\n",
        "\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "# Plot loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss, label='Training Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_acc, label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5etNqfHX5CS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics from Neural Network default and tuned models (excluding macro/weighted F1-scores)\n",
        "metrics = {\n",
        "    \"Metric\": [\n",
        "        \"Accuracy\",\n",
        "        \"AUC Score\",\n",
        "        \"Precision (Class 0)\",\n",
        "        \"Precision (Class 1)\",\n",
        "        \"Recall (Class 0)\",\n",
        "        \"Recall (Class 1)\",\n",
        "        \"F1-Score (Class 0)\",\n",
        "        \"F1-Score (Class 1)\",\n",
        "        \"True Negatives (TN)\",\n",
        "        \"False Positives (FP)\",\n",
        "        \"False Negatives (FN)\",\n",
        "        \"True Positives (TP)\"\n",
        "    ],\n",
        "    \"Default Model\": [\n",
        "        0.8559,\n",
        "        0.8696,\n",
        "        0.5127,\n",
        "        0.9571,\n",
        "        0.7790,\n",
        "        0.8695,\n",
        "        0.6184,\n",
        "        0.9112,\n",
        "        585,\n",
        "        166,\n",
        "        556,\n",
        "        3705\n",
        "    ],\n",
        "    \"Tuned Model\": [\n",
        "        0.8869,\n",
        "        0.8787,\n",
        "        0.6036,\n",
        "        0.9479,\n",
        "        0.7137,\n",
        "        0.9174,\n",
        "        0.6541,\n",
        "        0.9324,\n",
        "        536,\n",
        "        215,\n",
        "        352,\n",
        "        3909\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Compute percentage changes\n",
        "percent_change = []\n",
        "for orig, tuned in zip(metrics[\"Default Model\"], metrics[\"Tuned Model\"]):  # updated here\n",
        "    if orig == 0 or orig is None:\n",
        "        percent_change.append(None)\n",
        "    else:\n",
        "        change = ((tuned - orig) / orig) * 100\n",
        "        percent_change.append(change)\n",
        "\n",
        "# Create DataFrame\n",
        "df_nn = pd.DataFrame({\n",
        "    \"Metric\": metrics[\"Metric\"],\n",
        "    \"Default Model\": metrics[\"Default Model\"],  # updated here\n",
        "    \"Tuned Model\": metrics[\"Tuned Model\"],\n",
        "    \"% Change\": percent_change\n",
        "})\n",
        "\n",
        "# Style the DataFrame\n",
        "styled_df_nn = df_nn.style.format({\n",
        "    \"Default Model\": \"{:.4f}\",  # updated here\n",
        "    \"Tuned Model\": \"{:.4f}\",\n",
        "    \"% Change\": lambda x: \"\" if pd.isnull(x) else f\"{x:+.2f}%\"\n",
        "}).set_caption(\"Neural Network Performance Comparison\").set_properties(**{'text-align': 'center'})\n",
        "\n",
        "styled_df_nn"
      ],
      "metadata": {
        "id": "p4qDjZ6J5CS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "bars = plt.bar(df_nn[\"Metric\"], df_nn[\"% Change\"], color='skyblue')\n",
        "\n",
        "plt.axhline(0, color='gray', linewidth=0.8)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel(\"Percentage Change (%)\")\n",
        "plt.title(\"Percentage Change of Metrics: Tuned vs Original Neural Network Model\")\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.annotate(f'{height:+.2f}%',\n",
        "                 xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                 xytext=(0, 3),\n",
        "                 textcoords=\"offset points\",\n",
        "                 ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rwIY_FMe5CS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data for Neural Network including confusion matrix metrics\n",
        "nn_metrics = {\n",
        "    \"Metric\": [\n",
        "        \"Accuracy\",\n",
        "        \"AUC Score\",\n",
        "        \"Precision (Class 0)\",\n",
        "        \"Recall (Class 0)\",\n",
        "        \"F1-Score (Class 0)\",\n",
        "        \"Precision (Class 1)\",\n",
        "        \"Recall (Class 1)\",\n",
        "        \"F1-Score (Class 1)\",\n",
        "        \"True Negatives (TN)\",\n",
        "        \"False Positives (FP)\",\n",
        "        \"False Negatives (FN)\",\n",
        "        \"True Positives (TP)\"\n",
        "    ],\n",
        "    \"Default Model\": [\n",
        "        0.8559,\n",
        "        0.8696,\n",
        "        0.5127,\n",
        "        0.7790,\n",
        "        0.6184,\n",
        "        0.9571,\n",
        "        0.8695,\n",
        "        0.9112,\n",
        "        585,\n",
        "        166,\n",
        "        556,\n",
        "        3705\n",
        "    ],\n",
        "    \"Tuned Model\": [\n",
        "        0.8869,\n",
        "        0.8787,\n",
        "        0.6036,\n",
        "        0.7137,\n",
        "        0.6541,\n",
        "        0.9479,\n",
        "        0.9174,\n",
        "        0.9324,\n",
        "        536,\n",
        "        215,\n",
        "        352,\n",
        "        3909\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "# Data for XGBoost including confusion matrix metrics\n",
        "xgb_metrics = {\n",
        "    \"Metric\": [\n",
        "        \"Accuracy\",\n",
        "        \"AUC Score\",\n",
        "        \"Precision (Class 0)\",\n",
        "        \"Recall (Class 0)\",\n",
        "        \"F1-Score (Class 0)\",\n",
        "        \"Precision (Class 1)\",\n",
        "        \"Recall (Class 1)\",\n",
        "        \"F1-Score (Class 1)\",\n",
        "        \"True Negatives (TN)\",\n",
        "        \"False Positives (FP)\",\n",
        "        \"False Negatives (FN)\",\n",
        "        \"True Positives (TP)\"\n",
        "    ],\n",
        "    \"Default Model\": [\n",
        "        0.8659,\n",
        "        0.9012,\n",
        "        0.5400,\n",
        "        0.7800,\n",
        "        0.6400,\n",
        "        0.9600,\n",
        "        0.8800,\n",
        "        0.9200,\n",
        "        587,\n",
        "        164,\n",
        "        508,\n",
        "        3753\n",
        "    ],\n",
        "    \"Tuned Model\": [\n",
        "        0.8651,\n",
        "        0.8987,\n",
        "        0.5300,\n",
        "        0.7800,\n",
        "        0.6300,\n",
        "        0.9600,\n",
        "        0.8800,\n",
        "        0.9200,\n",
        "        585,\n",
        "        166,\n",
        "        510,\n",
        "        3751\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrames\n",
        "nn_df = pd.DataFrame(nn_metrics)\n",
        "xgb_df = pd.DataFrame(xgb_metrics)\n",
        "\n",
        "# Align metrics (in case order differs)\n",
        "common_metrics = list(set(nn_df[\"Metric\"]).intersection(xgb_df[\"Metric\"]))\n",
        "common_metrics.sort()\n",
        "\n",
        "# Filter DataFrames to only common metrics in the same order\n",
        "nn_df = nn_df[nn_df[\"Metric\"].isin(common_metrics)].set_index(\"Metric\").loc[common_metrics].reset_index()\n",
        "xgb_df = xgb_df[xgb_df[\"Metric\"].isin(common_metrics)].set_index(\"Metric\").loc[common_metrics].reset_index()\n",
        "\n",
        "# Function to compute percentage change: (NN - XGB) / XGB * 100\n",
        "def percentage_change(nn_values, xgb_values):\n",
        "    # To avoid division by zero, handle zero values\n",
        "    changes = []\n",
        "    for nn_val, xgb_val in zip(nn_values, xgb_values):\n",
        "        if xgb_val == 0:\n",
        "            changes.append(np.nan)\n",
        "        else:\n",
        "            changes.append(((nn_val - xgb_val) / xgb_val) * 100)\n",
        "    return changes\n",
        "\n",
        "# Table 1: Default Models Comparison\n",
        "default_compare = pd.DataFrame({\n",
        "    \"Metric\": common_metrics,\n",
        "    \"XGBoost Default\": xgb_df[\"Default Model\"].values,\n",
        "    \"Neural Net Default\": nn_df[\"Default Model\"].values\n",
        "})\n",
        "default_compare[\"% Change (NN vs XGB)\"] = percentage_change(default_compare[\"Neural Net Default\"], default_compare[\"XGBoost Default\"])\n",
        "\n",
        "# Table 2: Tuned Models Comparison\n",
        "tuned_compare = pd.DataFrame({\n",
        "    \"Metric\": common_metrics,\n",
        "    \"XGBoost Tuned\": xgb_df[\"Tuned Model\"].values,\n",
        "    \"Neural Net Tuned\": nn_df[\"Tuned Model\"].values\n",
        "})\n",
        "tuned_compare[\"% Change (NN vs XGB)\"] = percentage_change(tuned_compare[\"Neural Net Tuned\"], tuned_compare[\"XGBoost Tuned\"])\n",
        "\n",
        "# Format the percentage change column with +/– and 2 decimals for display\n",
        "def format_percentage_change(df, col_name):\n",
        "    return df.style.format({\n",
        "        col_name: \"{:+.2f}%\"\n",
        "    }).format(precision=4, subset=df.columns[1:-1]).set_properties(**{'text-align': 'center'})\n",
        "\n",
        "# Display tables nicely formatted\n",
        "print(\"Table 1: Default Models Comparison\")\n",
        "display(format_percentage_change(default_compare, \"% Change (NN vs XGB)\"))\n",
        "\n",
        "print(\"\\nTable 2: Tuned Models Comparison\")\n",
        "display(format_percentage_change(tuned_compare, \"% Change (NN vs XGB)\"))\n",
        "\n",
        "# Plot function for bar chart of percentage changes\n",
        "def plot_bar(df, title):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = plt.bar(df[\"Metric\"], df[\"% Change (NN vs XGB)\"], color='skyblue')\n",
        "    plt.axhline(0, color='gray', linewidth=0.8)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel(\"Percentage Change (%)\")\n",
        "    plt.title(title)\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        if not np.isnan(height):\n",
        "            plt.annotate(f'{height:+.2f}%',\n",
        "                         xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                         xytext=(0, 3),\n",
        "                         textcoords=\"offset points\",\n",
        "                         ha='center', va='bottom', fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot both bar charts\n",
        "plot_bar(default_compare, \"Percentage Change: Neural Network Default vs XGBoost Default\")\n",
        "plot_bar(tuned_compare, \"Percentage Change: Neural Network Tuned vs XGBoost Tuned\")\n"
      ],
      "metadata": {
        "id": "e5DtGk8m5CS8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}