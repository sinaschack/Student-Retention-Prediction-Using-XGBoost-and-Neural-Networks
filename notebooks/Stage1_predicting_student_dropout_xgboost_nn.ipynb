{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1"
      ],
      "metadata": {
        "id": "udPwf63_Zkgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import (roc_auc_score, accuracy_score,\n",
        "                              classification_report, precision_score,\n",
        "                              confusion_matrix, ConfusionMatrixDisplay)\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras import Input\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Install keras-tuner\n",
        "!pip install -q keras-tuner\n",
        "\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix, precision_score,\n",
        "    recall_score, roc_auc_score, classification_report\n",
        ")\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "!pip install optuna\n",
        "import optuna\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import optuna\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import recall_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall\n",
        "from sklearn.metrics import roc_curve"
      ],
      "metadata": {
        "id": "KMgDVBTUQx2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9u2r8g69lh3"
      },
      "outputs": [],
      "source": [
        "# File URL\n",
        "file_url = \"https://drive.google.com/uc?id=1pA8DDYmQuaLyxADCOZe1QaSQwF16q1J6\"\n",
        "\n",
        "# Loads the CSV file from Google Drive into a pandas DataFrame\n",
        "stage1_data = pd.read_csv(f\"https://drive.google.com/uc?export=download&id={file_url.split('=')[-1]}\")\n",
        "\n",
        "# View the first few rows\n",
        "stage1_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdIe7W4nlVhy"
      },
      "outputs": [],
      "source": [
        "# Check data types and missing values\n",
        "stage1_data.info()\n",
        "\n",
        "# Quick summary of data\n",
        "stage1_data.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17bxhHeWroH5"
      },
      "outputs": [],
      "source": [
        "# List all columns in the DataFrame\n",
        "for col in stage1_data.columns:\n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hxzU5iXvPoU"
      },
      "outputs": [],
      "source": [
        "# Drop 'LearnerCode' – it's just an ID, not predictive\n",
        "stage1_data.drop(columns=['LearnerCode'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_5PNJYJp0bW"
      },
      "outputs": [],
      "source": [
        "# Convert 'DateofBirth' to datetime\n",
        "stage1_data['DateofBirth'] = pd.to_datetime(stage1_data['DateofBirth'], errors='coerce', dayfirst=True)\n",
        "\n",
        "# Compute age assuming data collected in 2016\n",
        "stage1_data['Age'] = 2016 - stage1_data['DateofBirth'].dt.year\n",
        "\n",
        "# Why 2016?\n",
        "# A student born in 1998 is listed under Foundation, which is typically for students around 18 years old.\n",
        "# That suggests this record was collected around 2016 (1998 + 18).\n",
        "\n",
        "# Drop the original DateofBirth column\n",
        "stage1_data.drop(columns=['DateofBirth'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5NgNFS-tgS_"
      },
      "outputs": [],
      "source": [
        "# List all columns in the DataFrame\n",
        "for col in stage1_data.columns:\n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T38sg_swy9z"
      },
      "outputs": [],
      "source": [
        "# Identify columns with >200 unique values\n",
        "high_cardinality_cols = [col for col in stage1_data.columns if stage1_data[col].nunique() > 200]\n",
        "\n",
        "# Print columns that will be dropped\n",
        "print(\"Dropped columns due to high cardinality (>200 unique values):\")\n",
        "print(high_cardinality_cols)\n",
        "\n",
        "# Drop those columns\n",
        "stage1_data.drop(columns=high_cardinality_cols, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryIK0cE7rutF"
      },
      "outputs": [],
      "source": [
        "# List all columns in the DataFrame\n",
        "for col in stage1_data.columns:\n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbRLxS15xAlI"
      },
      "outputs": [],
      "source": [
        "# Save original column names\n",
        "original_columns = set(stage1_data.columns)\n",
        "\n",
        "# Drop columns where more than 50% of the data is missing\n",
        "threshold = len(stage1_data) * 0.5\n",
        "stage1_data.dropna(thresh=threshold, axis=1, inplace=True)\n",
        "\n",
        "# Save new column names\n",
        "remaining_columns = set(stage1_data.columns)\n",
        "\n",
        "# Find which columns were dropped\n",
        "dropped_columns = original_columns - remaining_columns\n",
        "print(\"Dropped columns due to >50% missing values:\", dropped_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FW6ufj7KiSd"
      },
      "outputs": [],
      "source": [
        "#from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Example: assuming 'stage1_data' is your DataFrame\n",
        "missing_percent = stage1_data.isnull().mean() * 100\n",
        "initial_row_count = len(stage1_data)\n",
        "\n",
        "# Dictionary to store dropped info\n",
        "dropped_info = {}\n",
        "\n",
        "# Drop rows if missing value is <2% in that column\n",
        "for col in stage1_data.columns:\n",
        "    if 0 < missing_percent[col] < 2:\n",
        "        missing_rows = stage1_data[col].isnull().sum()\n",
        "        dropped_info[col] = {\n",
        "            'rows_dropped': missing_rows,\n",
        "            'percent_of_total_rows': (missing_rows / initial_row_count) * 100\n",
        "        }\n",
        "        stage1_data = stage1_data[~stage1_data[col].isnull()]\n",
        "\n",
        "# Print the info\n",
        "if dropped_info:\n",
        "    print(\"Columns with dropped rows (missing < 2%):\")\n",
        "    for col, info in dropped_info.items():\n",
        "        print(f\"- {col}: {info['rows_dropped']} rows dropped \"\n",
        "              f\"({info['percent_of_total_rows']:.2f}% of total)\")\n",
        "else:\n",
        "    print(\"No columns had missing values <2%, so no rows were dropped.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = stage1_data.select_dtypes(include=np.number).columns\n",
        "if len(numeric_cols) > 0:\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    stage1_data[numeric_cols] = imputer.fit_transform(stage1_data[numeric_cols])\n",
        "    print(f\"Imputed missing values in numeric columns: {list(numeric_cols)}\")\n",
        "else:\n",
        "    print(\"No numeric columns found for imputation, skipping this step.\")"
      ],
      "metadata": {
        "id": "SdcU9llZKiSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9E8SfHVx2aw"
      },
      "outputs": [],
      "source": [
        "# Encode the target first, before it gets one-hot encoded\n",
        "stage1_data['CompletedCourse'] = stage1_data['CompletedCourse'].map({'Yes': 1, 'No': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjCRQz21xHGR"
      },
      "outputs": [],
      "source": [
        "# One-hot encode the other categorical features (excluding the target)\n",
        "categorical_cols = stage1_data.select_dtypes(include=['object', 'category']).columns\n",
        "categorical_cols = categorical_cols.drop('CompletedCourse', errors='ignore')\n",
        "\n",
        "stage1_data = pd.get_dummies(stage1_data, columns=categorical_cols, drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5E49mCJ6TTL"
      },
      "outputs": [],
      "source": [
        "# Count how many samples in each class\n",
        "print(stage1_data['CompletedCourse'].value_counts())\n",
        "\n",
        "# Same in %\n",
        "print(stage1_data['CompletedCourse'].value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHiyLD39v-lP"
      },
      "outputs": [],
      "source": [
        "# Plot histogram to check distribution\n",
        "stage1_data['CompletedCourse'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Distribution of CompletedCourse\")\n",
        "plt.xlabel(\"Completed (1 = Yes, 0 = No)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxwSAEFF4k8L"
      },
      "outputs": [],
      "source": [
        "# Split data into X (features) and y (target)\n",
        "X = stage1_data.drop(columns=['CompletedCourse'])\n",
        "y = stage1_data['CompletedCourse']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfxO_8Li4zJC"
      },
      "outputs": [],
      "source": [
        "# Split into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Have binary labels 0 and 1 -> can use scale_pos_weight as:\n",
        "num_neg = sum(y_train == 0)\n",
        "num_pos = sum(y_train == 1)\n",
        "scale_pos_weight = num_neg / num_pos"
      ],
      "metadata": {
        "id": "JvgciP9JQnNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybmXJ7VR5FII"
      },
      "outputs": [],
      "source": [
        "# Default XGBoost model (no tuning)\n",
        "\n",
        "model_xgb = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss')\n",
        "\n",
        "model_xgb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(15, 18))\n",
        "\n",
        "# Gain (average gain in accuracy) - top plot\n",
        "# How much a feature improves the model’s accuracy when it’s used for splitting.\n",
        "xgb.plot_importance(model_xgb, importance_type='gain', ax=axes[0], max_num_features=10, title='Feature Importance (Gain)')\n",
        "\n",
        "# Weight (frequency) - middle plot\n",
        "# Definition: How many times a feature is used to split nodes in all trees.\n",
        "xgb.plot_importance(model_xgb, importance_type='weight', ax=axes[1], max_num_features=10, title='Feature Importance (Weight)')\n",
        "\n",
        "# Cover (average coverage) - bottom plot\n",
        "# How many data points are affected by splits on that feature.\n",
        "xgb.plot_importance(model_xgb, importance_type='cover', ax=axes[2], max_num_features=10, title='Feature Importance (Cover)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N9uB8DRUfbp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get importance dictionaries\n",
        "gain_importance = model_xgb.get_booster().get_score(importance_type='gain')\n",
        "weight_importance = model_xgb.get_booster().get_score(importance_type='weight')\n",
        "cover_importance = model_xgb.get_booster().get_score(importance_type='cover')\n",
        "\n",
        "# Helper function to create sorted DataFrame for a given importance type\n",
        "def create_importance_df(importance_dict, top_n=10):\n",
        "    df = pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Value'])\n",
        "    df_sorted = df.sort_values(by='Value', ascending=False).head(top_n).reset_index(drop=True)\n",
        "    return df_sorted\n",
        "\n",
        "# Create tables\n",
        "gain_df = create_importance_df(gain_importance)\n",
        "weight_df = create_importance_df(weight_importance)\n",
        "cover_df = create_importance_df(cover_importance)\n",
        "\n",
        "# Display the tables\n",
        "print(\"Top 10 Features by Gain:\")\n",
        "print(gain_df, \"\\n\")\n",
        "\n",
        "print(\"Top 10 Features by Weight:\")\n",
        "print(weight_df, \"\\n\")\n",
        "\n",
        "print(\"Top 10 Features by Cover:\")\n",
        "print(cover_df)"
      ],
      "metadata": {
        "id": "pzqFBjcEod_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WABdVcPj1S04"
      },
      "outputs": [],
      "source": [
        "# Predictions\n",
        "y_pred_xgb = model_xgb.predict(X_test) # generates predicted labels for validation\n",
        "y_pred_proba_xgb = model_xgb.predict_proba(X_test)[:, 1] # gets predicted probabilities for the positive class\n",
        "\n",
        "# Metrics - measure how well the model’s predictions match the true values\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
        "print(f\"AUC Score: {roc_auc_score(y_test, y_pred_proba_xgb):.4f}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb, cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_xgb)\n",
        "auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT3jX9lI_26h"
      },
      "outputs": [],
      "source": [
        "# Split data into training and validation sets (80% train, 20% validation)\n",
        "# Preserving class distribution with stratification for balanced splits\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Optuna objective function\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "        'scale_pos_weight': scale_pos_weight,\n",
        "        'eval_metric': 'logloss',\n",
        "        'random_state': 42,\n",
        "        'use_label_encoder': False,\n",
        "        'verbosity': 0\n",
        "    }\n",
        "\n",
        "    model = XGBClassifier(**params)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    recall_scores = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        recall_0 = recall_score(y_val, preds, pos_label=0) # evaluate recall for class 0 after each fold and use it as the optimisation target\n",
        "        recall_scores.append(recall_0)\n",
        "\n",
        "    return np.mean(recall_scores)\n",
        "\n",
        "# Run the study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10) # limit to 10 because computationally expensive\n",
        "\n",
        "print(\"Best Hyperparameters from Optuna tuning:\")\n",
        "print(study.best_trial.params)\n",
        "print(f\"Best Mean Recall for Class 0 (Dropouts): {study.best_value:.4f}\")\n",
        "\n",
        "# Use best params from study\n",
        "best_params = study.best_trial.params\n",
        "best_params.update({\n",
        "    'scale_pos_weight': scale_pos_weight,\n",
        "    'eval_metric': 'logloss',\n",
        "    'random_state': 42,\n",
        "    'use_label_encoder': False,\n",
        "    'verbosity': 0\n",
        "})\n",
        "\n",
        "model_xgb_optimised = XGBClassifier(**best_params)\n",
        "\n",
        "# Train on full training data\n",
        "model_xgb_optimised.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate on test data\n",
        "y_pred_xgb = model_xgb_optimised.predict(X_test)\n",
        "y_pred_proba_xgb = model_xgb_optimised.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Metrics and confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_xgb)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
        "print(f\"AUC Score: {roc_auc_score(y_test, y_pred_proba_xgb):.4f}\")\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_xgb)\n",
        "auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pROPqHBI_66X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(3, 1, figsize=(15, 18))\n",
        "\n",
        "# Gain (average gain in accuracy) - top plot\n",
        "# How much a feature improves the model’s accuracy when it’s used for splitting.\n",
        "xgb.plot_importance(model_xgb, importance_type='gain', ax=axes[0], max_num_features=10, title='Feature Importance (Gain)')\n",
        "\n",
        "# Weight (frequency) - middle plot\n",
        "# Definition: How many times a feature is used to split nodes in all trees.\n",
        "xgb.plot_importance(model_xgb, importance_type='weight', ax=axes[1], max_num_features=10, title='Feature Importance (Weight)')\n",
        "\n",
        "# Cover (average coverage) - bottom plot\n",
        "# How many data points are affected by splits on that feature.\n",
        "xgb.plot_importance(model_xgb, importance_type='cover', ax=axes[2], max_num_features=10, title='Feature Importance (Cover)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "utbqEkYrkdJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get importance dictionaries\n",
        "gain_importance = model_xgb.get_booster().get_score(importance_type='gain')\n",
        "weight_importance = model_xgb.get_booster().get_score(importance_type='weight')\n",
        "cover_importance = model_xgb.get_booster().get_score(importance_type='cover')\n",
        "\n",
        "# Helper function to create sorted DataFrame for a given importance type\n",
        "def create_importance_df(importance_dict, top_n=10):\n",
        "    df = pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Value'])\n",
        "    df_sorted = df.sort_values(by='Value', ascending=False).head(top_n).reset_index(drop=True)\n",
        "    return df_sorted\n",
        "\n",
        "# Create tables\n",
        "gain_df = create_importance_df(gain_importance)\n",
        "weight_df = create_importance_df(weight_importance)\n",
        "cover_df = create_importance_df(cover_importance)\n",
        "\n",
        "# Display the tables\n",
        "print(\"Top 10 Features by Gain:\")\n",
        "print(gain_df, \"\\n\")\n",
        "\n",
        "print(\"Top 10 Features by Weight:\")\n",
        "print(weight_df, \"\\n\")\n",
        "\n",
        "print(\"Top 10 Features by Cover:\")\n",
        "print(cover_df)"
      ],
      "metadata": {
        "id": "XYvVTXUFojnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics from both models (default and tuned)\n",
        "metrics_data = {\n",
        "    'Metric': [\n",
        "        'Accuracy',\n",
        "        'AUC Score',\n",
        "        'Precision (Class 0)',\n",
        "        'Precision (Class 1)',\n",
        "        'Recall (Class 0)',\n",
        "        'Recall (Class 1)',\n",
        "        'F1-Score (Class 0)',\n",
        "        'F1-Score (Class 1)',\n",
        "        'True Negatives (TN)',\n",
        "        'False Positives (FP)',\n",
        "        'False Negatives (FN)',\n",
        "        'True Positives (TP)'\n",
        "    ],\n",
        "    'Default Model': [\n",
        "        0.8553,\n",
        "        0.8785,\n",
        "        0.51,\n",
        "        0.96,\n",
        "        0.77,\n",
        "        0.87,\n",
        "        0.61,\n",
        "        0.91,\n",
        "        578,\n",
        "        173,\n",
        "        552,\n",
        "        3709\n",
        "    ],\n",
        "    'Tuned Model': [\n",
        "        0.8538,\n",
        "        0.8798,\n",
        "        0.51,\n",
        "        0.96,\n",
        "        0.7796,\n",
        "        0.87,\n",
        "        0.61,\n",
        "        0.91,\n",
        "        578,\n",
        "        173,\n",
        "        560,\n",
        "        3701\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "# Calculate percentage change from default to tuned\n",
        "def pct_change(default, tuned):\n",
        "    return np.where(default != 0, (tuned - default) / default * 100, np.nan)\n",
        "\n",
        "metrics_df['Percentage Change (%)'] = pct_change(metrics_df['Default Model'], metrics_df['Tuned Model'])\n",
        "\n",
        "# Format display: 4 decimals for models, + sign for percentage changes\n",
        "styled_df = metrics_df.style.format({\n",
        "    'Default Model': '{:.4f}',\n",
        "    'Tuned Model': '{:.4f}',\n",
        "    'Percentage Change (%)': '{:+.2f}%'\n",
        "}).set_caption(\"Model Performance Comparison with Percentage Change\").set_properties(**{'text-align': 'center'})\n",
        "\n",
        "styled_df"
      ],
      "metadata": {
        "id": "NRSAeOktbypH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Bar plot for percentage changes using your metrics_df DataFrame\n",
        "bars = plt.bar(metrics_df[\"Metric\"], metrics_df[\"Percentage Change (%)\"], color='skyblue')\n",
        "\n",
        "\n",
        "plt.axhline(0, color='gray', linewidth=0.8)\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.ylabel(\"Percentage Change (%)\")\n",
        "plt.title(\"Percentage Change of Metrics: Tuned vs Default XGBoost\")\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.annotate(f'{height:+.2f}%',\n",
        "                 xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                 xytext=(0, 3),\n",
        "                 textcoords=\"offset points\",\n",
        "                 ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gdlSTDu1E_2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decided to scale the data for better convergence\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "vWS3xIb9Ecbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handles class imbalance\n",
        "# Cares more about underrepresented class by giving it a higher penalty when misclassified\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "3-veF8xc68dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'), # 64 neruons are moderate complexity/ relu makes training fast\n",
        "    Dropout(0.3), # Prevents overfitting by randomly dropping 30% of neurons\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2), # Drops 20% to reduce overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "FSlie5237Z7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring how the model will learn during training\n",
        "\n",
        "# Track recall for class 0 (negative class)\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy', # telling the model to track both: (1) Overall accuracy,\n",
        "        Recall(name='recall_0', class_id=0) # and (2)  recall specifically for class 0 (the negative class) during training and evaluation.\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "QTB9Jokt8agr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    class_weight=class_weight_dict,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "ScaPAUbe8vKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_probs = model.predict(X_test_scaled)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "_1hv1zB79hPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "test_loss, test_accuracy, test_recall_0 = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Classification Report (Precision, Recall, F1)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# AUC\n",
        "auc = roc_auc_score(y_test, y_pred_probs)\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "plt.figure(figsize=(6, 6))\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC Curve\n",
        "# Receiver Operating Characteristic curve\n",
        "# ROC curve plots True Positive Rate vs. False Positive Rate at various thresholds\n",
        "# AUC is the Area Under the ROC Curve — a summary measure of the classifier’s overall ability to distinguish classes\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "94CelyDp90Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Loss and Accuracy Curves\n",
        "\n",
        "# Extract values from the training history\n",
        "history_dict = history.history\n",
        "\n",
        "train_loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "train_acc = history_dict.get('accuracy')\n",
        "val_acc = history_dict.get('val_accuracy')\n",
        "\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "# Plot loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss, label='Training Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_acc, label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DP6UScOB_GDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model-building function for Keras Tuner\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(X_train_scaled.shape[1],)))\n",
        "\n",
        "    # Tune number of units in first Dense layer\n",
        "    units = hp.Choice('units', values=[32, 64, 128], default=64)\n",
        "    model.add(Dense(units, activation=hp.Choice('activation', ['relu', 'tanh'], default='tanh'))) # Tune activation function to find the best nonlinearity for the data\n",
        "    model.add(Dropout(hp.Float('dropout_1', 0.1, 0.5, step=0.1, default=0.3)))\n",
        "\n",
        "    # Second Dense layer adds capacity if needed (flexibility to increase complexity)\n",
        "    if hp.Boolean('second_layer', default=True):\n",
        "        units2 = hp.Int('units_2', min_value=16, max_value=64, step=16, default=32)\n",
        "        model.add(Dense(units2, activation=hp.Choice('activation_2', ['relu', 'tanh'], default='tanh')))\n",
        "        model.add(Dropout(hp.Float('dropout_2', 0.1, 0.3, step=0.1, default=0.2))) # The dropout ranges (0.1 to 0.5) help address potential overfitting seen in the fluctuating validation loss.\n",
        "        # Additional dropout for the second layer to control overfitting\n",
        "        dropout_2 = hp.Float('dropout_2', 0.1, 0.3, step=0.1, default=0.2)\n",
        "        model.add(Dropout(dropout_2))\n",
        "    # Output layer with sigmoid activation for binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Tune learning rate (1e-4 to 1e-2) to optimize training convergence and stability\n",
        "    lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=lr),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "    # With the use of compiled Keras Sequential models, the functionality can be configured\n",
        "    # with different tunable layers, activations, dropout, and learning rates, which can then lead to training."
      ],
      "metadata": {
        "id": "zrV2e0gVILXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the best hyperparameters by running multiple model configurations\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy', # maximize validation accuracy\n",
        "    max_trials=10, # 15 was used as a starting point but reduced the number of trials to save time, but fewer trials may limit the search quality\n",
        "    executions_per_trial=1,\n",
        "    directory='hyperparam_tuning',\n",
        "    project_name='nn_imbalanced_data'\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=30, # Number of epochs to train each trial\n",
        "    batch_size=32, # Number of samples per gradient update\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    class_weight=class_weight_dict, # Weights to handle class imbalance during training\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "OUxaprzCIYH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best model found by the tuner\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_probs = best_model.predict(X_test_scaled).ravel()\n",
        "y_pred = (y_pred_probs > 0.5).astype(int) # Convert probabilities to binary class predictions using 0.5 threshold\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, digits=4)\n",
        "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best tuned model accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "print(f\"AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2yDmK3N-ORx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Loss and Accuracy Curves\n",
        "\n",
        "# Extract values from the training history\n",
        "history_dict = history.history\n",
        "\n",
        "train_loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "train_acc = history_dict.get('accuracy')  # Might be 'acc' in older Keras\n",
        "val_acc = history_dict.get('val_accuracy')\n",
        "\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "# Plot loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss, label='Training Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_acc, label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WOZOesAN-sjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics from Neural Network default and tuned models (excluding macro/weighted F1-scores)\n",
        "metrics = {\n",
        "    \"Metric\": [\n",
        "        \"Accuracy\",\n",
        "        \"AUC Score\",\n",
        "        \"Precision (Class 0)\",\n",
        "        \"Precision (Class 1)\",\n",
        "        \"Recall (Class 0)\",\n",
        "        \"Recall (Class 1)\",\n",
        "        \"F1-Score (Class 0)\",\n",
        "        \"F1-Score (Class 1)\",\n",
        "        \"True Negatives (TN)\",\n",
        "        \"False Positives (FP)\",\n",
        "        \"False Negatives (FN)\",\n",
        "        \"True Positives (TP)\"\n",
        "    ],\n",
        "    \"Default Model\": [  # renamed here\n",
        "        0.8619,\n",
        "        0.8482,\n",
        "        0.5281,\n",
        "        0.9503,\n",
        "        0.7377,\n",
        "        0.8838,\n",
        "        0.6156,\n",
        "        0.9159,\n",
        "        554,\n",
        "        197,\n",
        "        495,\n",
        "        3766\n",
        "    ],\n",
        "    \"Tuned Model\": [\n",
        "        0.8675,\n",
        "        0.8481,\n",
        "        0.5429,\n",
        "        0.9500,\n",
        "        0.7337,\n",
        "        0.8911,\n",
        "        0.6240,\n",
        "        0.9196,\n",
        "        551,\n",
        "        200,\n",
        "        464,\n",
        "        3797\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Compute percentage changes\n",
        "percent_change = []\n",
        "for orig, tuned in zip(metrics[\"Default Model\"], metrics[\"Tuned Model\"]):  # updated here\n",
        "    if orig == 0 or orig is None:\n",
        "        percent_change.append(None)\n",
        "    else:\n",
        "        change = ((tuned - orig) / orig) * 100\n",
        "        percent_change.append(change)\n",
        "\n",
        "# Create DataFrame\n",
        "df_nn = pd.DataFrame({\n",
        "    \"Metric\": metrics[\"Metric\"],\n",
        "    \"Default Model\": metrics[\"Default Model\"],  # updated here\n",
        "    \"Tuned Model\": metrics[\"Tuned Model\"],\n",
        "    \"% Change\": percent_change\n",
        "})\n",
        "\n",
        "# Style the DataFrame\n",
        "styled_df_nn = df_nn.style.format({\n",
        "    \"Default Model\": \"{:.4f}\",  # updated here\n",
        "    \"Tuned Model\": \"{:.4f}\",\n",
        "    \"% Change\": lambda x: \"\" if pd.isnull(x) else f\"{x:+.2f}%\"\n",
        "}).set_caption(\"Neural Network Performance Comparison\").set_properties(**{'text-align': 'center'})\n",
        "\n",
        "styled_df_nn"
      ],
      "metadata": {
        "id": "k4t5VVDEOk_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "bars = plt.bar(df_nn[\"Metric\"], df_nn[\"% Change\"], color='skyblue')\n",
        "\n",
        "plt.axhline(0, color='gray', linewidth=0.8)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel(\"Percentage Change (%)\")\n",
        "plt.title(\"Percentage Change of Metrics: Tuned vs Original Neural Network Model\")\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.annotate(f'{height:+.2f}%',\n",
        "                 xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                 xytext=(0, 3),\n",
        "                 textcoords=\"offset points\",\n",
        "                 ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jx44SdtnED75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data for Neural Network including confusion matrix metrics\n",
        "nn_metrics = {\n",
        "    \"Metric\": [\n",
        "        \"Accuracy\",\n",
        "        \"AUC Score\",\n",
        "        \"Precision (Class 0)\",\n",
        "        \"Recall (Class 0)\",\n",
        "        \"F1-Score (Class 0)\",\n",
        "        \"Precision (Class 1)\",\n",
        "        \"Recall (Class 1)\",\n",
        "        \"F1-Score (Class 1)\",\n",
        "        \"True Negatives (TN)\",\n",
        "        \"False Positives (FP)\",\n",
        "        \"False Negatives (FN)\",\n",
        "        \"True Positives (TP)\"\n",
        "    ],\n",
        "    \"Default Model\": [\n",
        "        0.8619,\n",
        "        0.8482,\n",
        "        0.5281,\n",
        "        0.7377,\n",
        "        0.6156,\n",
        "        0.9503,\n",
        "        0.8838,\n",
        "        0.9159,\n",
        "        554,\n",
        "        197,\n",
        "        495,\n",
        "        3766\n",
        "    ],\n",
        "    \"Tuned Model\": [\n",
        "        0.8675,\n",
        "        0.8481,\n",
        "        0.5429,\n",
        "        0.7337,\n",
        "        0.6240,\n",
        "        0.9500,\n",
        "        0.8911,\n",
        "        0.9196,\n",
        "        551,\n",
        "        200,\n",
        "        464,\n",
        "        3797\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Data for XGBoost including confusion matrix metrics\n",
        "xgb_metrics = {\n",
        "    \"Metric\": [\n",
        "        \"Accuracy\",\n",
        "        \"AUC Score\",\n",
        "        \"Precision (Class 0)\",\n",
        "        \"Recall (Class 0)\",\n",
        "        \"F1-Score (Class 0)\",\n",
        "        \"Precision (Class 1)\",\n",
        "        \"Recall (Class 1)\",\n",
        "        \"F1-Score (Class 1)\",\n",
        "        \"True Negatives (TN)\",\n",
        "        \"False Positives (FP)\",\n",
        "        \"False Negatives (FN)\",\n",
        "        \"True Positives (TP)\"\n",
        "    ],\n",
        "    \"Default Model\": [\n",
        "        0.8553,\n",
        "        0.8785,\n",
        "        0.51,\n",
        "        0.77,\n",
        "        0.61,\n",
        "        0.96,\n",
        "        0.87,\n",
        "        0.91,\n",
        "        578,\n",
        "        173,\n",
        "        552,\n",
        "        3709\n",
        "    ],\n",
        "    \"Tuned Model\": [\n",
        "        0.8538,\n",
        "        0.8798,\n",
        "        0.51,\n",
        "        0.7796,\n",
        "        0.61,\n",
        "        0.96,\n",
        "        0.87,\n",
        "        0.91,\n",
        "        578,\n",
        "        173,\n",
        "        560,\n",
        "        3701\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrames\n",
        "nn_df = pd.DataFrame(nn_metrics)\n",
        "xgb_df = pd.DataFrame(xgb_metrics)\n",
        "\n",
        "# Align metrics (in case order differs)\n",
        "common_metrics = list(set(nn_df[\"Metric\"]).intersection(xgb_df[\"Metric\"]))\n",
        "common_metrics.sort()\n",
        "\n",
        "# Filter DataFrames to only common metrics in the same order\n",
        "nn_df = nn_df[nn_df[\"Metric\"].isin(common_metrics)].set_index(\"Metric\").loc[common_metrics].reset_index()\n",
        "xgb_df = xgb_df[xgb_df[\"Metric\"].isin(common_metrics)].set_index(\"Metric\").loc[common_metrics].reset_index()\n",
        "\n",
        "# Function to compute percentage change: (NN - XGB) / XGB * 100\n",
        "def percentage_change(nn_values, xgb_values):\n",
        "    # To avoid division by zero, handle zero values\n",
        "    changes = []\n",
        "    for nn_val, xgb_val in zip(nn_values, xgb_values):\n",
        "        if xgb_val == 0:\n",
        "            changes.append(np.nan)\n",
        "        else:\n",
        "            changes.append(((nn_val - xgb_val) / xgb_val) * 100)\n",
        "    return changes\n",
        "\n",
        "# Table 1: Default Models Comparison\n",
        "default_compare = pd.DataFrame({\n",
        "    \"Metric\": common_metrics,\n",
        "    \"XGBoost Default\": xgb_df[\"Default Model\"].values,\n",
        "    \"Neural Net Default\": nn_df[\"Default Model\"].values\n",
        "})\n",
        "default_compare[\"% Change (NN vs XGB)\"] = percentage_change(default_compare[\"Neural Net Default\"], default_compare[\"XGBoost Default\"])\n",
        "\n",
        "# Table 2: Tuned Models Comparison\n",
        "tuned_compare = pd.DataFrame({\n",
        "    \"Metric\": common_metrics,\n",
        "    \"XGBoost Tuned\": xgb_df[\"Tuned Model\"].values,\n",
        "    \"Neural Net Tuned\": nn_df[\"Tuned Model\"].values\n",
        "})\n",
        "tuned_compare[\"% Change (NN vs XGB)\"] = percentage_change(tuned_compare[\"Neural Net Tuned\"], tuned_compare[\"XGBoost Tuned\"])\n",
        "\n",
        "# Format the percentage change column with +/– and 2 decimals for display\n",
        "def format_percentage_change(df, col_name):\n",
        "    return df.style.format({\n",
        "        col_name: \"{:+.2f}%\"\n",
        "    }).format(precision=4, subset=df.columns[1:-1]).set_properties(**{'text-align': 'center'})\n",
        "\n",
        "# Display tables nicely formatted\n",
        "print(\"Table 1: Default Models Comparison\")\n",
        "display(format_percentage_change(default_compare, \"% Change (NN vs XGB)\"))\n",
        "\n",
        "print(\"\\nTable 2: Tuned Models Comparison\")\n",
        "display(format_percentage_change(tuned_compare, \"% Change (NN vs XGB)\"))\n",
        "\n",
        "# Plot function for bar chart of percentage changes\n",
        "def plot_bar(df, title):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = plt.bar(df[\"Metric\"], df[\"% Change (NN vs XGB)\"], color='skyblue')\n",
        "    plt.axhline(0, color='gray', linewidth=0.8)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel(\"Percentage Change (%)\")\n",
        "    plt.title(title)\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        if not np.isnan(height):\n",
        "            plt.annotate(f'{height:+.2f}%',\n",
        "                         xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                         xytext=(0, 3),\n",
        "                         textcoords=\"offset points\",\n",
        "                         ha='center', va='bottom', fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot both bar charts\n",
        "plot_bar(default_compare, \"Percentage Change: Neural Network Default vs XGBoost Default\")\n",
        "plot_bar(tuned_compare, \"Percentage Change: Neural Network Tuned vs XGBoost Tuned\")"
      ],
      "metadata": {
        "id": "mm1nUl89ITtl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
