{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 3"
      ],
      "metadata": {
        "id": "wMe8QiJB_Kwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File URL\n",
        "file_url_3 = \"https://drive.google.com/uc?id=18oyu-RQotQN6jaibsLBoPdqQJbj_cV2-\"\n",
        "\n",
        "# Loads the CSV file from Google Drive into a pandas DataFrame\n",
        "stage3_data = pd.read_csv(f\"https://drive.google.com/uc?export=download&id={file_url_3.split('=')[-1]}\")\n",
        "\n",
        "# View the first few rows\n",
        "stage3_data.head()"
      ],
      "metadata": {
        "id": "i0Rigjfl-e5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmmdAZG6l1fN"
      },
      "outputs": [],
      "source": [
        "# Check data types and missing values\n",
        "stage3_data.info()\n",
        "\n",
        "# Quick summary of data\n",
        "stage3_data.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NhQCqiNl1fR"
      },
      "outputs": [],
      "source": [
        "# Drop 'LearnerCode' – it's just an ID, not predictive\n",
        "stage3_data.drop(columns=['LearnerCode'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plKgGbocl1fS"
      },
      "outputs": [],
      "source": [
        "# Convert 'DateofBirth' to datetime\n",
        "stage3_data['DateofBirth'] = pd.to_datetime(stage3_data['DateofBirth'], errors='coerce', dayfirst=True)\n",
        "\n",
        "# Compute age assuming data collected in 2016\n",
        "stage3_data['Age'] = 2016 - stage3_data['DateofBirth'].dt.year\n",
        "\n",
        "# Why 2016?\n",
        "# A student born in 1998 is listed under Foundation, which is typically for students around 18 years old.\n",
        "# That suggests this record was collected around 2016 (1998 + 18).\n",
        "\n",
        "# Drop the original DateofBirth column\n",
        "stage3_data.drop(columns=['DateofBirth'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUi97rVTl1fT"
      },
      "outputs": [],
      "source": [
        "# List all columns in the DataFrame\n",
        "for col in stage3_data.columns:\n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfR3ohG6l1fU"
      },
      "outputs": [],
      "source": [
        "# Identify columns with >200 unique values\n",
        "high_cardinality_cols = [col for col in stage3_data.columns if stage3_data[col].nunique() > 200]\n",
        "\n",
        "# Drop those columns\n",
        "stage3_data.drop(columns=high_cardinality_cols, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLoqBZb3l1fV"
      },
      "outputs": [],
      "source": [
        "# List all columns in the DataFrame\n",
        "for col in stage3_data.columns:\n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWfXIA4Rl1fW"
      },
      "outputs": [],
      "source": [
        "# Save original column names\n",
        "original_columns = set(stage3_data.columns)\n",
        "\n",
        "# Drop columns where more than 50% of the data is missing\n",
        "threshold = len(stage3_data) * 0.5\n",
        "stage3_data.dropna(thresh=threshold, axis=1, inplace=True)\n",
        "\n",
        "# Save new column names\n",
        "remaining_columns = set(stage3_data.columns)\n",
        "\n",
        "# Find which columns were dropped\n",
        "dropped_columns = original_columns - remaining_columns\n",
        "print(\"Dropped columns due to >50% missing values:\", dropped_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Xs_LRZJLLFe"
      },
      "outputs": [],
      "source": [
        "#from sklearn.impute import SimpleImputer\n",
        "\n",
        "missing_percent = stage3_data.isnull().mean() * 100\n",
        "initial_row_count = len(stage3_data)\n",
        "\n",
        "# Dictionary to store dropped info\n",
        "dropped_info = {}\n",
        "\n",
        "# Drop rows if missing value is <2% in that column\n",
        "for col in stage3_data.columns:\n",
        "    if 0 < missing_percent[col] < 2:\n",
        "        missing_rows = stage3_data[col].isnull().sum()\n",
        "        dropped_info[col] = {\n",
        "            'rows_dropped': missing_rows,\n",
        "            'percent_of_total_rows': (missing_rows / initial_row_count) * 100\n",
        "        }\n",
        "        stage3_data = stage3_data[~stage3_data[col].isnull()]\n",
        "\n",
        "# Print the info\n",
        "if dropped_info:\n",
        "    print(\"Columns with dropped rows (missing < 2%):\")\n",
        "    for col, info in dropped_info.items():\n",
        "        print(f\"- {col}: {info['rows_dropped']} rows dropped \"\n",
        "              f\"({info['percent_of_total_rows']:.2f}% of total)\")\n",
        "else:\n",
        "    print(\"No columns had missing values <2%, so no rows were dropped.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = stage3_data.select_dtypes(include=np.number).columns\n",
        "if len(numeric_cols) > 0:\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    stage3_data[numeric_cols] = imputer.fit_transform(stage3_data[numeric_cols])\n",
        "    print(f\"Imputed missing values in numeric columns: {list(numeric_cols)}\")\n",
        "else:\n",
        "    print(\"No numeric columns found for imputation, skipping this step.\")"
      ],
      "metadata": {
        "id": "ODAwG_uiLLFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erWA1ed5l1fY"
      },
      "outputs": [],
      "source": [
        "# Encode the target first, before it gets one-hot encoded\n",
        "stage3_data['CompletedCourse'] = stage3_data['CompletedCourse'].map({'Yes': 1, 'No': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrAAb6-Il1fZ"
      },
      "outputs": [],
      "source": [
        "# One-hot encode the other categorical features (excluding the target)\n",
        "categorical_cols = stage3_data.select_dtypes(include=['object', 'category']).columns\n",
        "categorical_cols = categorical_cols.drop('CompletedCourse', errors='ignore')\n",
        "\n",
        "stage3_data = pd.get_dummies(stage3_data, columns=categorical_cols, drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4Gube7el1fa"
      },
      "outputs": [],
      "source": [
        "# Count how many samples in each class\n",
        "print(stage3_data['CompletedCourse'].value_counts())\n",
        "\n",
        "# Same in %\n",
        "print(stage3_data['CompletedCourse'].value_counts(normalize=True) * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJL7Yx-Yl1fa"
      },
      "outputs": [],
      "source": [
        "# Plot histogram to check distribution\n",
        "stage3_data['CompletedCourse'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Distribution of CompletedCourse\")\n",
        "plt.xlabel(\"Completed (1 = Yes, 0 = No)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDf-QfvIl1fb"
      },
      "outputs": [],
      "source": [
        "# Split data into X (features) and y (target)\n",
        "X = stage3_data.drop(columns=['CompletedCourse'])\n",
        "y = stage3_data['CompletedCourse']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuQqY9F3l1fb"
      },
      "outputs": [],
      "source": [
        "# Split into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Have binary labels 0 and 1 -> can use scale_pos_weight as:\n",
        "num_neg = sum(y_train == 0)\n",
        "num_pos = sum(y_train == 1)\n",
        "scale_pos_weight = num_neg / num_pos"
      ],
      "metadata": {
        "id": "yyTguEyMgjoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_xgb = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss')\n",
        "\n",
        "model_xgb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "KwKpzFpxgjoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(3, 1, figsize=(15, 18))\n",
        "\n",
        "# Gain (average gain in accuracy) - top plot\n",
        "# How much a feature improves the model’s accuracy when it’s used for splitting.\n",
        "xgb.plot_importance(model_xgb, importance_type='gain', ax=axes[0], max_num_features=10, title='Feature Importance (Gain)')\n",
        "\n",
        "# Weight (frequency) - middle plot\n",
        "# Definition: How many times a feature is used to split nodes in all trees.\n",
        "xgb.plot_importance(model_xgb, importance_type='weight', ax=axes[1], max_num_features=10, title='Feature Importance (Weight)')\n",
        "\n",
        "# Cover (average coverage) - bottom plot\n",
        "# How many data points are affected by splits on that feature.\n",
        "xgb.plot_importance(model_xgb, importance_type='cover', ax=axes[2], max_num_features=10, title='Feature Importance (Cover)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ltqp3p9NgjoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get importance dictionaries\n",
        "gain_importance = model_xgb.get_booster().get_score(importance_type='gain')\n",
        "weight_importance = model_xgb.get_booster().get_score(importance_type='weight')\n",
        "cover_importance = model_xgb.get_booster().get_score(importance_type='cover')\n",
        "\n",
        "# Helper function to create sorted DataFrame for a given importance type\n",
        "def create_importance_df(importance_dict, top_n=10):\n",
        "    df = pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Value'])\n",
        "    df_sorted = df.sort_values(by='Value', ascending=False).head(top_n).reset_index(drop=True)\n",
        "    return df_sorted\n",
        "\n",
        "# Create tables\n",
        "gain_df = create_importance_df(gain_importance)\n",
        "weight_df = create_importance_df(weight_importance)\n",
        "cover_df = create_importance_df(cover_importance)\n",
        "\n",
        "# Display the tables\n",
        "print(\"Top 10 Features by Gain:\")\n",
        "print(gain_df, \"\\n\")\n",
        "\n",
        "print(\"Top 10 Features by Weight:\")\n",
        "print(weight_df, \"\\n\")\n",
        "\n",
        "print(\"Top 10 Features by Cover:\")\n",
        "print(cover_df)"
      ],
      "metadata": {
        "id": "dXw1eQfGgjoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_xgb = model_xgb.predict(X_test)\n",
        "y_pred_proba_xgb = model_xgb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Metrics\n",
        "cm = confusion_matrix(y_test, y_pred_xgb)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
        "print(f\"AUC Score: {roc_auc_score(y_test, y_pred_proba_xgb):.4f}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_xgb)\n",
        "auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vloLbv3QgjoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets (80% train, 20% validation)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "HvSto6uUgjoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Optuna objective function\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "        'scale_pos_weight': scale_pos_weight,\n",
        "        'eval_metric': 'logloss',\n",
        "        'random_state': 42,\n",
        "        'use_label_encoder': False,\n",
        "        'verbosity': 0\n",
        "    }\n",
        "\n",
        "    model = XGBClassifier(**params)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    recall_scores = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        recall_0 = recall_score(y_val, preds, pos_label=0)\n",
        "        recall_scores.append(recall_0)\n",
        "\n",
        "    return np.mean(recall_scores)\n",
        "\n",
        "# Run the study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"Best Hyperparameters from Optuna tuning:\")\n",
        "print(study.best_trial.params)\n",
        "print(f\"Best Mean Recall for Class 0 (Dropouts): {study.best_value:.4f}\")\n",
        "\n",
        "# Use best params from study\n",
        "best_params = study.best_trial.params\n",
        "best_params.update({\n",
        "    'scale_pos_weight': scale_pos_weight,\n",
        "    'eval_metric': 'logloss',\n",
        "    'random_state': 42,\n",
        "    'use_label_encoder': False,\n",
        "    'verbosity': 0\n",
        "})\n",
        "\n",
        "model_xgb_optimised = XGBClassifier(**best_params)\n",
        "\n",
        "# Train on full training data\n",
        "model_xgb_optimised.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate on test data\n",
        "y_pred_xgb = model_xgb_optimised.predict(X_test)\n",
        "y_pred_proba_xgb = model_xgb_optimised.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Metrics and confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_xgb)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
        "print(f\"AUC Score: {roc_auc_score(y_test, y_pred_proba_xgb):.4f}\")\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TM_AfB6OgjoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_xgb)\n",
        "auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XfNh1kkigjoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(3, 1, figsize=(15, 18))\n",
        "\n",
        "# Gain (average gain in accuracy) - top plot\n",
        "# How much a feature improves the model’s accuracy when it’s used for splitting.\n",
        "xgb.plot_importance(model_xgb, importance_type='gain', ax=axes[0], max_num_features=10, title='Feature Importance (Gain)')\n",
        "\n",
        "# Weight (frequency) - middle plot\n",
        "# Definition: How many times a feature is used to split nodes in all trees.\n",
        "xgb.plot_importance(model_xgb, importance_type='weight', ax=axes[1], max_num_features=10, title='Feature Importance (Weight)')\n",
        "\n",
        "# Cover (average coverage) - bottom plot\n",
        "# How many data points are affected by splits on that feature.\n",
        "xgb.plot_importance(model_xgb, importance_type='cover', ax=axes[2], max_num_features=10, title='Feature Importance (Cover)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UZJDZSpAgjoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get importance dictionaries\n",
        "gain_importance = model_xgb.get_booster().get_score(importance_type='gain')\n",
        "weight_importance = model_xgb.get_booster().get_score(importance_type='weight')\n",
        "cover_importance = model_xgb.get_booster().get_score(importance_type='cover')\n",
        "\n",
        "# Helper function to create sorted DataFrame for a given importance type\n",
        "def create_importance_df(importance_dict, top_n=10):\n",
        "    df = pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Value'])\n",
        "    df_sorted = df.sort_values(by='Value', ascending=False).head(top_n).reset_index(drop=True)\n",
        "    return df_sorted\n",
        "\n",
        "# Create tables\n",
        "gain_df = create_importance_df(gain_importance)\n",
        "weight_df = create_importance_df(weight_importance)\n",
        "cover_df = create_importance_df(cover_importance)\n",
        "\n",
        "# Display the tables\n",
        "print(\"Top 10 Features by Gain:\")\n",
        "print(gain_df, \"\\n\")\n",
        "\n",
        "print(\"Top 10 Features by Weight:\")\n",
        "print(weight_df, \"\\n\")\n",
        "\n",
        "print(\"Top 10 Features by Cover:\")\n",
        "print(cover_df)"
      ],
      "metadata": {
        "id": "6S-Nu1-WgjoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics from both models (default and tuned)\n",
        "metrics_data = {\n",
        "    'Metric': [\n",
        "        'Accuracy',\n",
        "        'AUC Score',\n",
        "        'Precision (Class 0)',\n",
        "        'Precision (Class 1)',\n",
        "        'Recall (Class 0)',\n",
        "        'Recall (Class 1)',\n",
        "        'F1-Score (Class 0)',\n",
        "        'F1-Score (Class 1)',\n",
        "        'True Negatives (TN)',\n",
        "        'False Positives (FP)',\n",
        "        'False Negatives (FN)',\n",
        "        'True Positives (TP)'\n",
        "    ],\n",
        "    'Default Model': [\n",
        "        0.9869,\n",
        "        0.9992,\n",
        "        0.94,\n",
        "        1.00,\n",
        "        0.98,\n",
        "        0.99,\n",
        "        0.96,\n",
        "        0.99,\n",
        "        705,\n",
        "        17,\n",
        "        48,\n",
        "        4201\n",
        "    ],\n",
        "    'Tuned Model': [\n",
        "        0.9835,\n",
        "        0.9989,\n",
        "        0.91,\n",
        "        1.00,\n",
        "        0.99,\n",
        "        0.98,\n",
        "        0.95,\n",
        "        0.99,\n",
        "        712,\n",
        "        10,\n",
        "        72,\n",
        "        4177\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "# Calculate percentage change from default to tuned\n",
        "def pct_change(default, tuned):\n",
        "    return np.where(default != 0, (tuned - default) / default * 100, np.nan)\n",
        "\n",
        "metrics_df['Percentage Change (%)'] = pct_change(metrics_df['Default Model'], metrics_df['Tuned Model'])\n",
        "\n",
        "# Format display: 4 decimals for models, + sign for percentage changes\n",
        "styled_df = metrics_df.style.format({\n",
        "    'Default Model': '{:.4f}',\n",
        "    'Tuned Model': '{:.4f}',\n",
        "    'Percentage Change (%)': '{:+.2f}%'\n",
        "}).set_caption(\"Model Performance Comparison with Percentage Change\").set_properties(**{'text-align': 'center'})\n",
        "\n",
        "styled_df"
      ],
      "metadata": {
        "id": "8TFBUSFsgjoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Bar plot for percentage changes using your metrics_df DataFrame\n",
        "bars = plt.bar(metrics_df[\"Metric\"], metrics_df[\"Percentage Change (%)\"], color='skyblue')\n",
        "\n",
        "\n",
        "plt.axhline(0, color='gray', linewidth=0.8)\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.ylabel(\"Percentage Change (%)\")\n",
        "plt.title(\"Percentage Change of Metrics: Tuned vs Default XGBoost\")\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.annotate(f'{height:+.2f}%',\n",
        "                 xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                 xytext=(0, 3),\n",
        "                 textcoords=\"offset points\",\n",
        "                 ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "60dCRLxrgjoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decided to scale the data for better convergence\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "RJpXiJCggjoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handles class imbalance\n",
        "# Cares more about underrepresented class by giving it a higher penalty when misclassified\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "pJwSlRo7gjoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'), # 64 neruons are moderate complexity/ relu makes training fast\n",
        "    Dropout(0.3), # Prevents overfitting by randomly dropping 30% of neurons\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2), # Drops 20% to reduce overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "56jcGbiIgjoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "model.compile(\n",
        "    optimizer=Adam(), # handles the imbalanced classes well\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "4cdYPBmagjoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    class_weight=class_weight_dict,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "OZmhPxQ2gjoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_probs = model.predict(X_test_scaled)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "0Dymz7q2gjoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Classification Report (Precision, Recall, F1)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# AUC\n",
        "auc = roc_auc_score(y_test, y_pred_probs)\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5dlpTbxogjoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Loss and Accuracy Curves\n",
        "\n",
        "# Extract values from the training history\n",
        "history_dict = history.history\n",
        "\n",
        "train_loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "train_acc = history_dict.get('accuracy')  # Might be 'acc' in older Keras\n",
        "val_acc = history_dict.get('val_accuracy')\n",
        "\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "# Plot loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss, label='Training Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_acc, label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4B5N6GslgjoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model-building function for Keras Tuner\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(X_train_scaled.shape[1],)))\n",
        "\n",
        "    # Tune number of units in first Dense layer\n",
        "    units = hp.Choice('units', values=[32, 64, 128], default=64)\n",
        "    model.add(Dense(units, activation=hp.Choice('activation', ['relu', 'tanh'], default='tanh'))) # Tune activation function to find the best nonlinearity for the data\n",
        "    model.add(Dropout(hp.Float('dropout_1', 0.1, 0.5, step=0.1, default=0.3)))\n",
        "\n",
        "    # Second Dense layer adds capacity if needed (flexibility to increase complexity)\n",
        "    if hp.Boolean('second_layer', default=True):\n",
        "        units2 = hp.Int('units_2', min_value=16, max_value=64, step=16, default=32)\n",
        "        model.add(Dense(units2, activation=hp.Choice('activation_2', ['relu', 'tanh'], default='tanh')))\n",
        "        model.add(Dropout(hp.Float('dropout_2', 0.1, 0.3, step=0.1, default=0.2))) # The dropout ranges (0.1 to 0.5) help address potential overfitting seen in the fluctuating validation loss.\n",
        "        # Additional dropout for the second layer to control overfitting\n",
        "        dropout_2 = hp.Float('dropout_2', 0.1, 0.3, step=0.1, default=0.2)\n",
        "        model.add(Dropout(dropout_2))\n",
        "    # Output layer with sigmoid activation for binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Tune learning rate (1e-4 to 1e-2) to optimize training convergence and stability\n",
        "    lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=lr),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "    # With the use of compiled Keras Sequential models, the functionality can be configured\n",
        "    # with different tunable layers, activations, dropout, and learning rates, which can then lead to training."
      ],
      "metadata": {
        "id": "52MugxyPgjoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the best hyperparameters by running multiple model configurations\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy', # maximize validation accuracy\n",
        "    max_trials=10, # 15 was used as a starting point but reduced the number of trials to save time, but fewer trials may limit the search quality\n",
        "    executions_per_trial=1,\n",
        "    directory='hyperparam_tuning',\n",
        "    project_name='nn_imbalanced_data'\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=30, # Number of epochs to train each trial\n",
        "    batch_size=32, # Number of samples per gradient update\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    class_weight=class_weight_dict, # Weights to handle class imbalance during training\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "DX2JmfGngjoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best model found by the tuner\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_probs = best_model.predict(X_test_scaled).ravel()\n",
        "y_pred = (y_pred_probs > 0.5).astype(int) # Convert probabilities to binary class predictions using 0.5 threshold\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, digits=4)\n",
        "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best tuned model accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "print(f\"AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NVFt52wEgjoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Loss and Accuracy Curves\n",
        "\n",
        "# Extract values from the training history\n",
        "history_dict = history.history\n",
        "\n",
        "train_loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "train_acc = history_dict.get('accuracy')  # Might be 'acc' in older Keras\n",
        "val_acc = history_dict.get('val_accuracy')\n",
        "\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "# Plot loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss, label='Training Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_acc, label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bGM__KabgjoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics from Neural Network default and tuned models (excluding macro/weighted F1-scores)\n",
        "metrics = {\n",
        "    \"Metric\": [\n",
        "        \"Accuracy\",\n",
        "        \"AUC Score\",\n",
        "        \"Precision (Class 0)\",\n",
        "        \"Precision (Class 1)\",\n",
        "        \"Recall (Class 0)\",\n",
        "        \"Recall (Class 1)\",\n",
        "        \"F1-Score (Class 0)\",\n",
        "        \"F1-Score (Class 1)\",\n",
        "        \"True Negatives (TN)\",\n",
        "        \"False Positives (FP)\",\n",
        "        \"False Negatives (FN)\",\n",
        "        \"True Positives (TP)\"\n",
        "    ],\n",
        "    \"Default Model\": [\n",
        "        0.9773,\n",
        "        0.9783,\n",
        "        0.9166,\n",
        "        0.9877,\n",
        "        0.9280,\n",
        "        0.9856,\n",
        "        0.9222,\n",
        "        0.9867,\n",
        "        670,\n",
        "        52,\n",
        "        61,\n",
        "        4188\n",
        "    ],\n",
        "    \"Tuned Model\": [\n",
        "        0.8869,\n",
        "        0.8787,\n",
        "        0.6036,\n",
        "        0.9479,\n",
        "        0.7137,\n",
        "        0.9174,\n",
        "        0.6541,\n",
        "        0.9324,\n",
        "        536,\n",
        "        215,\n",
        "        352,\n",
        "        3909\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Compute percentage changes\n",
        "percent_change = []\n",
        "for orig, tuned in zip(metrics[\"Default Model\"], metrics[\"Tuned Model\"]):  # updated here\n",
        "    if orig == 0 or orig is None:\n",
        "        percent_change.append(None)\n",
        "    else:\n",
        "        change = ((tuned - orig) / orig) * 100\n",
        "        percent_change.append(change)\n",
        "\n",
        "# Create DataFrame\n",
        "df_nn = pd.DataFrame({\n",
        "    \"Metric\": metrics[\"Metric\"],\n",
        "    \"Default Model\": metrics[\"Default Model\"],  # updated here\n",
        "    \"Tuned Model\": metrics[\"Tuned Model\"],\n",
        "    \"% Change\": percent_change\n",
        "})\n",
        "\n",
        "# Style the DataFrame\n",
        "styled_df_nn = df_nn.style.format({\n",
        "    \"Default Model\": \"{:.4f}\",  # updated here\n",
        "    \"Tuned Model\": \"{:.4f}\",\n",
        "    \"% Change\": lambda x: \"\" if pd.isnull(x) else f\"{x:+.2f}%\"\n",
        "}).set_caption(\"Neural Network Performance Comparison\").set_properties(**{'text-align': 'center'})\n",
        "\n",
        "styled_df_nn"
      ],
      "metadata": {
        "id": "dbkGM-SLgjoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "bars = plt.bar(df_nn[\"Metric\"], df_nn[\"% Change\"], color='skyblue')\n",
        "\n",
        "plt.axhline(0, color='gray', linewidth=0.8)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel(\"Percentage Change (%)\")\n",
        "plt.title(\"Percentage Change of Metrics: Tuned vs Original Neural Network Model\")\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.annotate(f'{height:+.2f}%',\n",
        "                 xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                 xytext=(0, 3),\n",
        "                 textcoords=\"offset points\",\n",
        "                 ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZyMQLjqtgjoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data for Neural Network including confusion matrix metrics\n",
        "nn_metrics = {\n",
        "    \"Metric\": [\n",
        "        \"Accuracy\",\n",
        "        \"AUC Score\",\n",
        "        \"Precision (Class 0)\",\n",
        "        \"Recall (Class 0)\",\n",
        "        \"F1-Score (Class 0)\",\n",
        "        \"Precision (Class 1)\",\n",
        "        \"Recall (Class 1)\",\n",
        "        \"F1-Score (Class 1)\",\n",
        "        \"True Negatives (TN)\",\n",
        "        \"False Positives (FP)\",\n",
        "        \"False Negatives (FN)\",\n",
        "        \"True Positives (TP)\"\n",
        "    ],\n",
        "    \"Default Model\": [\n",
        "        0.9773,\n",
        "        0.9783,\n",
        "        0.9166,\n",
        "        0.9280,\n",
        "        0.9222,\n",
        "        0.9877,\n",
        "        0.9856,\n",
        "        0.9867,\n",
        "        670,\n",
        "        52,\n",
        "        61,\n",
        "        4188\n",
        "    ],\n",
        "    \"Tuned Model\": [\n",
        "        0.8869,\n",
        "        0.8787,\n",
        "        0.6036,\n",
        "        0.7137,\n",
        "        0.6541,\n",
        "        0.9479,\n",
        "        0.9174,\n",
        "        0.9324,\n",
        "        536,\n",
        "        215,\n",
        "        352,\n",
        "        3909\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Data for XGBoost including confusion matrix metrics\n",
        "xgb_metrics = {\n",
        "    \"Metric\": [\n",
        "        \"Accuracy\",\n",
        "        \"AUC Score\",\n",
        "        \"Precision (Class 0)\",\n",
        "        \"Recall (Class 0)\",\n",
        "        \"F1-Score (Class 0)\",\n",
        "        \"Precision (Class 1)\",\n",
        "        \"Recall (Class 1)\",\n",
        "        \"F1-Score (Class 1)\",\n",
        "        \"True Negatives (TN)\",\n",
        "        \"False Positives (FP)\",\n",
        "        \"False Negatives (FN)\",\n",
        "        \"True Positives (TP)\"\n",
        "    ],\n",
        "    \"Default Model\": [\n",
        "        0.9869,\n",
        "        0.9992,\n",
        "        0.9400,\n",
        "        0.9800,\n",
        "        0.9600,\n",
        "        1.0000,\n",
        "        0.9900,\n",
        "        0.9900,\n",
        "        705,\n",
        "        17,\n",
        "        48,\n",
        "        4201\n",
        "    ],\n",
        "    \"Tuned Model\": [\n",
        "        0.9835,\n",
        "        0.9989,\n",
        "        0.9100,\n",
        "        0.9900,\n",
        "        0.9500,\n",
        "        1.0000,\n",
        "        0.9800,\n",
        "        0.9900,\n",
        "        712,\n",
        "        10,\n",
        "        72,\n",
        "        4177\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrames\n",
        "nn_df = pd.DataFrame(nn_metrics)\n",
        "xgb_df = pd.DataFrame(xgb_metrics)\n",
        "\n",
        "# Align metrics (in case order differs)\n",
        "common_metrics = list(set(nn_df[\"Metric\"]).intersection(xgb_df[\"Metric\"]))\n",
        "common_metrics.sort()\n",
        "\n",
        "# Filter DataFrames to only common metrics in the same order\n",
        "nn_df = nn_df[nn_df[\"Metric\"].isin(common_metrics)].set_index(\"Metric\").loc[common_metrics].reset_index()\n",
        "xgb_df = xgb_df[xgb_df[\"Metric\"].isin(common_metrics)].set_index(\"Metric\").loc[common_metrics].reset_index()\n",
        "\n",
        "# Function to compute percentage change: (NN - XGB) / XGB * 100\n",
        "def percentage_change(nn_values, xgb_values):\n",
        "    # To avoid division by zero, handle zero values\n",
        "    changes = []\n",
        "    for nn_val, xgb_val in zip(nn_values, xgb_values):\n",
        "        if xgb_val == 0:\n",
        "            changes.append(np.nan)\n",
        "        else:\n",
        "            changes.append(((nn_val - xgb_val) / xgb_val) * 100)\n",
        "    return changes\n",
        "\n",
        "# Table 1: Default Models Comparison\n",
        "default_compare = pd.DataFrame({\n",
        "    \"Metric\": common_metrics,\n",
        "    \"XGBoost Default\": xgb_df[\"Default Model\"].values,\n",
        "    \"Neural Net Default\": nn_df[\"Default Model\"].values\n",
        "})\n",
        "default_compare[\"% Change (NN vs XGB)\"] = percentage_change(default_compare[\"Neural Net Default\"], default_compare[\"XGBoost Default\"])\n",
        "\n",
        "# Table 2: Tuned Models Comparison\n",
        "tuned_compare = pd.DataFrame({\n",
        "    \"Metric\": common_metrics,\n",
        "    \"XGBoost Tuned\": xgb_df[\"Tuned Model\"].values,\n",
        "    \"Neural Net Tuned\": nn_df[\"Tuned Model\"].values\n",
        "})\n",
        "tuned_compare[\"% Change (NN vs XGB)\"] = percentage_change(tuned_compare[\"Neural Net Tuned\"], tuned_compare[\"XGBoost Tuned\"])\n",
        "\n",
        "# Format the percentage change column with +/– and 2 decimals for display\n",
        "def format_percentage_change(df, col_name):\n",
        "    return df.style.format({\n",
        "        col_name: \"{:+.2f}%\"\n",
        "    }).format(precision=4, subset=df.columns[1:-1]).set_properties(**{'text-align': 'center'})\n",
        "\n",
        "# Display tables nicely formatted\n",
        "print(\"Table 1: Default Models Comparison\")\n",
        "display(format_percentage_change(default_compare, \"% Change (NN vs XGB)\"))\n",
        "\n",
        "print(\"\\nTable 2: Tuned Models Comparison\")\n",
        "display(format_percentage_change(tuned_compare, \"% Change (NN vs XGB)\"))\n",
        "\n",
        "# Plot function for bar chart of percentage changes\n",
        "def plot_bar(df, title):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = plt.bar(df[\"Metric\"], df[\"% Change (NN vs XGB)\"], color='skyblue')\n",
        "    plt.axhline(0, color='gray', linewidth=0.8)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel(\"Percentage Change (%)\")\n",
        "    plt.title(title)\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        if not np.isnan(height):\n",
        "            plt.annotate(f'{height:+.2f}%',\n",
        "                         xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                         xytext=(0, 3),\n",
        "                         textcoords=\"offset points\",\n",
        "                         ha='center', va='bottom', fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot both bar charts\n",
        "plot_bar(default_compare, \"Percentage Change: Neural Network Default vs XGBoost Default\")\n",
        "plot_bar(tuned_compare, \"Percentage Change: Neural Network Tuned vs XGBoost Tuned\")\n"
      ],
      "metadata": {
        "id": "1pH0IfrDgjoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated stage labels for only Mid-stage and Late-stage\n",
        "stages = ['Mid-stage', 'Late-stage']\n",
        "\n",
        "# Corresponding accuracy values only for stages 2 and 3 (index 1 and 2)\n",
        "xgb_default = [0.8659, 0.9869]\n",
        "xgb_tuned = [0.8651, 0.9835]\n",
        "nn_default = [0.8559, 0.9773]\n",
        "nn_tuned = [0.8869, 0.8869]\n",
        "\n",
        "# Bar width and positions\n",
        "bar_width = 0.2\n",
        "x = np.arange(len(stages))\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "ax.bar(x - 1.5 * bar_width, xgb_default, width=bar_width, label='XGBoost Default')\n",
        "ax.bar(x - 0.5 * bar_width, xgb_tuned, width=bar_width, label='XGBoost Tuned')\n",
        "ax.bar(x + 0.5 * bar_width, nn_default, width=bar_width, label='Neural Network Default')\n",
        "ax.bar(x + 1.5 * bar_width, nn_tuned, width=bar_width, label='Neural Network Tuned')\n",
        "\n",
        "# Labels and formatting\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Accuracy Comparison for Mid-stage and Late-stage')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(stages)\n",
        "ax.set_ylim(0.8, 1.0)\n",
        "ax.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YrJoS3hMcnsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated stage labels for only Mid-stage and Late-stage\n",
        "stages = ['Mid-stage', 'Late-stage']\n",
        "\n",
        "# Corresponding AUC values only for stages 2 and 3 (index 1 and 2)\n",
        "xgb_default_auc = [0.9012, 0.9992]\n",
        "xgb_tuned_auc = [0.8987, 0.9989]\n",
        "nn_default_auc = [0.8696, 0.9783]\n",
        "nn_tuned_auc = [0.8787, 0.8787]\n",
        "\n",
        "# Bar width and positions\n",
        "bar_width = 0.2\n",
        "x = np.arange(len(stages))\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "ax.bar(x - 1.5 * bar_width, xgb_default_auc, width=bar_width, label='XGBoost Default')\n",
        "ax.bar(x - 0.5 * bar_width, xgb_tuned_auc, width=bar_width, label='XGBoost Tuned')\n",
        "ax.bar(x + 0.5 * bar_width, nn_default_auc, width=bar_width, label='Neural Network Default')\n",
        "ax.bar(x + 1.5 * bar_width, nn_tuned_auc, width=bar_width, label='Neural Network Tuned')\n",
        "\n",
        "# Labels and formatting\n",
        "ax.set_ylabel('AUC Score')\n",
        "ax.set_title('AUC Score Comparison for Mid-stage and Late-stage')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(stages)\n",
        "ax.set_ylim(0.8, 1.05)\n",
        "ax.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1XO8vJPMdJLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated stage labels for Mid-stage and Late-stage only\n",
        "stages = ['Mid-stage', 'Late-stage']\n",
        "\n",
        "# Precision values for Class 0 (only indices 1 and 2)\n",
        "xgb_default_prec_0 = [0.5400, 0.9400]\n",
        "xgb_tuned_prec_0   = [0.5300, 0.9100]\n",
        "nn_default_prec_0  = [0.5127, 0.9166]\n",
        "nn_tuned_prec_0    = [0.6036, 0.6036]\n",
        "\n",
        "# Precision values for Class 1 (only indices 1 and 2)\n",
        "xgb_default_prec_1 = [0.9600, 1.0000]\n",
        "xgb_tuned_prec_1   = [0.9600, 1.0000]\n",
        "nn_default_prec_1  = [0.9571, 0.9877]\n",
        "nn_tuned_prec_1    = [0.9479, 0.9479]\n",
        "\n",
        "# Bar width and positions\n",
        "bar_width = 0.1\n",
        "x = np.arange(len(stages))\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Grouped bars for Class 0\n",
        "ax.bar(x - 3.5 * bar_width, xgb_default_prec_0, width=bar_width, label='XGB Default (Class 0)')\n",
        "ax.bar(x - 2.5 * bar_width, xgb_tuned_prec_0, width=bar_width, label='XGB Tuned (Class 0)')\n",
        "ax.bar(x - 1.5 * bar_width, nn_default_prec_0, width=bar_width, label='NN Default (Class 0)')\n",
        "ax.bar(x - 0.5 * bar_width, nn_tuned_prec_0, width=bar_width, label='NN Tuned (Class 0)')\n",
        "\n",
        "# Grouped bars for Class 1 (with hatch and alpha)\n",
        "ax.bar(x + 0.5 * bar_width, xgb_default_prec_1, width=bar_width, label='XGB Default (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 1.5 * bar_width, xgb_tuned_prec_1, width=bar_width, label='XGB Tuned (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 2.5 * bar_width, nn_default_prec_1, width=bar_width, label='NN Default (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 3.5 * bar_width, nn_tuned_prec_1, width=bar_width, label='NN Tuned (Class 1)', alpha=0.6, hatch='//')\n",
        "\n",
        "# Labels and formatting\n",
        "ax.set_ylabel('Precision')\n",
        "ax.set_title('Precision Comparison for Mid-stage and Late-stage Across Models and Classes')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(stages)\n",
        "ax.set_ylim(0.4, 1.05)\n",
        "ax.legend(ncol=2)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1q7k_Or_djGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated stage labels for Mid-stage and Late-stage only\n",
        "stages = ['Mid-stage', 'Late-stage']\n",
        "\n",
        "# Recall values for Class 0 (indices 1 and 2)\n",
        "xgb_default_recall_0 = [0.7800, 0.9800]\n",
        "xgb_tuned_recall_0   = [0.7800, 0.9900]\n",
        "nn_default_recall_0  = [0.7790, 0.9280]\n",
        "nn_tuned_recall_0    = [0.7137, 0.7137]\n",
        "\n",
        "# Recall values for Class 1 (indices 1 and 2)\n",
        "xgb_default_recall_1 = [0.8800, 0.9900]\n",
        "xgb_tuned_recall_1   = [0.8800, 0.9800]\n",
        "nn_default_recall_1  = [0.8695, 0.9856]\n",
        "nn_tuned_recall_1    = [0.9174, 0.9174]\n",
        "\n",
        "# Bar width and positions\n",
        "bar_width = 0.1\n",
        "x = np.arange(len(stages))\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Class 0 Recall bars\n",
        "ax.bar(x - 3.5 * bar_width, xgb_default_recall_0, width=bar_width, label='XGB Default (Class 0)')\n",
        "ax.bar(x - 2.5 * bar_width, xgb_tuned_recall_0, width=bar_width, label='XGB Tuned (Class 0)')\n",
        "ax.bar(x - 1.5 * bar_width, nn_default_recall_0, width=bar_width, label='NN Default (Class 0)')\n",
        "ax.bar(x - 0.5 * bar_width, nn_tuned_recall_0, width=bar_width, label='NN Tuned (Class 0)')\n",
        "\n",
        "# Class 1 Recall bars (hatch and transparency)\n",
        "ax.bar(x + 0.5 * bar_width, xgb_default_recall_1, width=bar_width, label='XGB Default (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 1.5 * bar_width, xgb_tuned_recall_1, width=bar_width, label='XGB Tuned (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 2.5 * bar_width, nn_default_recall_1, width=bar_width, label='NN Default (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 3.5 * bar_width, nn_tuned_recall_1, width=bar_width, label='NN Tuned (Class 1)', alpha=0.6, hatch='//')\n",
        "\n",
        "# Labels and formatting\n",
        "ax.set_ylabel('Recall')\n",
        "ax.set_title('Recall Comparison for Mid-stage and Late-stage Across Models and Classes')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(stages)\n",
        "ax.set_ylim(0.6, 1.05)\n",
        "ax.legend(ncol=2)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-nFgsi0reBua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated stage labels for Mid-stage and Late-stage only\n",
        "stages = ['Mid-stage', 'Late-stage']\n",
        "cm_metrics = ['TN', 'FP', 'FN', 'TP']\n",
        "\n",
        "# Confusion matrix data filtered for stages 1 and 2\n",
        "xgb_default_cm = {\n",
        "    'TN': [587, 705],\n",
        "    'FP': [164, 17],\n",
        "    'FN': [508, 48],\n",
        "    'TP': [3753, 4201]\n",
        "}\n",
        "\n",
        "xgb_tuned_cm = {\n",
        "    'TN': [585, 712],\n",
        "    'FP': [166, 10],\n",
        "    'FN': [510, 72],\n",
        "    'TP': [3751, 4177]\n",
        "}\n",
        "\n",
        "nn_default_cm = {\n",
        "    'TN': [585, 670],\n",
        "    'FP': [166, 52],\n",
        "    'FN': [556, 61],\n",
        "    'TP': [3705, 4188]\n",
        "}\n",
        "\n",
        "nn_tuned_cm = {\n",
        "    'TN': [536, 536],\n",
        "    'FP': [215, 215],\n",
        "    'FN': [352, 352],\n",
        "    'TP': [3909, 3909]\n",
        "}\n",
        "\n",
        "bar_width = 0.18\n",
        "x = np.arange(len(stages))\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(16, 10), sharex=True)\n",
        "\n",
        "# Full names for metrics for titles\n",
        "full_names = {\n",
        "    'TN': 'True Negatives (TN)',\n",
        "    'FP': 'False Positives (FP)',\n",
        "    'FN': 'False Negatives (FN)',\n",
        "    'TP': 'True Positives (TP)'\n",
        "}\n",
        "\n",
        "for i, metric in enumerate(cm_metrics):\n",
        "    ax = axs[i // 2, i % 2]\n",
        "\n",
        "    ax.bar(x - 1.5*bar_width, xgb_default_cm[metric], width=bar_width, label='XGB Default')\n",
        "    ax.bar(x - 0.5*bar_width, xgb_tuned_cm[metric], width=bar_width, label='XGB Tuned')\n",
        "    ax.bar(x + 0.5*bar_width, nn_default_cm[metric], width=bar_width, label='NN Default')\n",
        "    ax.bar(x + 1.5*bar_width, nn_tuned_cm[metric], width=bar_width, label='NN Tuned')\n",
        "\n",
        "    ax.set_title(full_names[metric])\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(stages)\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    if i % 2 == 0:\n",
        "        ax.set_ylabel('Count')\n",
        "\n",
        "fig.suptitle('Confusion Matrix Components Comparison for Mid-stage and Late-stage Across Models', fontsize=16)\n",
        "fig.legend(loc='upper right', bbox_to_anchor=(0.95, 0.95))\n",
        "plt.tight_layout(rect=[0, 0, 0.9, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "htGlJvnpeeWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated stages for Mid-stage and Late-stage only\n",
        "stages = ['Mid-stage', 'Late-stage']\n",
        "\n",
        "# Filtered F1-Score values for stages 1 and 2\n",
        "xgb_default_f1_0 = [0.6400, 0.9600]\n",
        "xgb_tuned_f1_0   = [0.6300, 0.9500]\n",
        "nn_default_f1_0  = [0.6184, 0.9222]\n",
        "nn_tuned_f1_0    = [0.6541, 0.6541]\n",
        "\n",
        "xgb_default_f1_1 = [0.9200, 0.9900]\n",
        "xgb_tuned_f1_1   = [0.9200, 0.9900]\n",
        "nn_default_f1_1  = [0.9112, 0.9867]\n",
        "nn_tuned_f1_1    = [0.9324, 0.9324]\n",
        "\n",
        "bar_width = 0.1\n",
        "x = np.arange(len(stages))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Class 0 F1-Score bars\n",
        "ax.bar(x - 3.5 * bar_width, xgb_default_f1_0, width=bar_width, label='XGB Default (Class 0)')\n",
        "ax.bar(x - 2.5 * bar_width, xgb_tuned_f1_0, width=bar_width, label='XGB Tuned (Class 0)')\n",
        "ax.bar(x - 1.5 * bar_width, nn_default_f1_0, width=bar_width, label='NN Default (Class 0)')\n",
        "ax.bar(x - 0.5 * bar_width, nn_tuned_f1_0, width=bar_width, label='NN Tuned (Class 0)')\n",
        "\n",
        "# Class 1 F1-Score bars (hatch and transparency)\n",
        "ax.bar(x + 0.5 * bar_width, xgb_default_f1_1, width=bar_width, label='XGB Default (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 1.5 * bar_width, xgb_tuned_f1_1, width=bar_width, label='XGB Tuned (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 2.5 * bar_width, nn_default_f1_1, width=bar_width, label='NN Default (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 3.5 * bar_width, nn_tuned_f1_1, width=bar_width, label='NN Tuned (Class 1)', alpha=0.6, hatch='//')\n",
        "\n",
        "# Labels and formatting\n",
        "ax.set_ylabel('F1-Score')\n",
        "ax.set_title('F1-Score Comparison for Mid-stage and Late-stage Across Models and Classes')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(stages)\n",
        "ax.set_ylim(0.5, 1.05)\n",
        "ax.legend(ncol=2)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wif-tsOse44N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated stage labels\n",
        "stages = ['Early-stage', 'Mid-stage', 'Late-stage']\n",
        "xgb_default = [0.8553, 0.8659, 0.9869]\n",
        "xgb_tuned = [0.8538, 0.8651, 0.9835]\n",
        "nn_default = [0.8619, 0.8559, 0.9773]\n",
        "nn_tuned = [0.8675, 0.8869, 0.8869]\n",
        "\n",
        "# Bar width and positions\n",
        "bar_width = 0.2\n",
        "x = np.arange(len(stages))\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.bar(x - 1.5 * bar_width, xgb_default, width=bar_width, label='XGBoost Default')\n",
        "ax.bar(x - 0.5 * bar_width, xgb_tuned, width=bar_width, label='XGBoost Tuned')\n",
        "ax.bar(x + 0.5 * bar_width, nn_default, width=bar_width, label='Neural Network Default')\n",
        "ax.bar(x + 1.5 * bar_width, nn_tuned, width=bar_width, label='Neural Network Tuned')\n",
        "\n",
        "# Labels and formatting\n",
        "#ax.set_xlabel('Stage')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Accuracy Comparison Across Stages and Models')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(stages)\n",
        "ax.set_ylim(0.8, 1.0)\n",
        "ax.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vpjUtZIaWFHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated stage labels\n",
        "stages = ['Early-stage', 'Mid-stage', 'Late-stage']\n",
        "\n",
        "# AUC values from your data\n",
        "xgb_default_auc = [0.8785, 0.9012, 0.9992]\n",
        "xgb_tuned_auc = [0.8798, 0.8987, 0.9989]\n",
        "nn_default_auc = [0.8482, 0.8696, 0.9783]\n",
        "nn_tuned_auc = [0.8481, 0.8787, 0.8787]\n",
        "\n",
        "# Bar width and positions\n",
        "bar_width = 0.2\n",
        "x = np.arange(len(stages))\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.bar(x - 1.5 * bar_width, xgb_default_auc, width=bar_width, label='XGBoost Default')\n",
        "ax.bar(x - 0.5 * bar_width, xgb_tuned_auc, width=bar_width, label='XGBoost Tuned')\n",
        "ax.bar(x + 0.5 * bar_width, nn_default_auc, width=bar_width, label='Neural Network Default')\n",
        "ax.bar(x + 1.5 * bar_width, nn_tuned_auc, width=bar_width, label='Neural Network Tuned')\n",
        "\n",
        "# Labels and formatting\n",
        "# ax.set_xlabel('Stage')\n",
        "ax.set_ylabel('AUC Score')\n",
        "ax.set_title('AUC Score Comparison Across Stages and Models')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(stages)\n",
        "ax.set_ylim(0.8, 1.05)\n",
        "ax.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NtmkwhmvW6u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated stage labels\n",
        "stages = ['Early-stage', 'Mid-stage', 'Late-stage']\n",
        "\n",
        "# Precision values for Class 0\n",
        "xgb_default_prec_0 = [0.5100, 0.5400, 0.9400]\n",
        "xgb_tuned_prec_0   = [0.5100, 0.5300, 0.9100]\n",
        "nn_default_prec_0  = [0.5281, 0.5127, 0.9166]\n",
        "nn_tuned_prec_0    = [0.5429, 0.6036, 0.6036]\n",
        "\n",
        "# Precision values for Class 1\n",
        "xgb_default_prec_1 = [0.9600, 0.9600, 1.0000]\n",
        "xgb_tuned_prec_1   = [0.9600, 0.9600, 1.0000]\n",
        "nn_default_prec_1  = [0.9503, 0.9571, 0.9877]\n",
        "nn_tuned_prec_1    = [0.9500, 0.9479, 0.9479]\n",
        "\n",
        "# Bar width and positions\n",
        "bar_width = 0.1\n",
        "x = np.arange(len(stages))\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Grouped bars\n",
        "ax.bar(x - 3.5 * bar_width, xgb_default_prec_0, width=bar_width, label='XGB Default (Class 0)')\n",
        "ax.bar(x - 2.5 * bar_width, xgb_tuned_prec_0, width=bar_width, label='XGB Tuned (Class 0)')\n",
        "ax.bar(x - 1.5 * bar_width, nn_default_prec_0, width=bar_width, label='NN Default (Class 0)')\n",
        "ax.bar(x - 0.5 * bar_width, nn_tuned_prec_0, width=bar_width, label='NN Tuned (Class 0)')\n",
        "\n",
        "ax.bar(x + 0.5 * bar_width, xgb_default_prec_1, width=bar_width, label='XGB Default (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 1.5 * bar_width, xgb_tuned_prec_1, width=bar_width, label='XGB Tuned (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 2.5 * bar_width, nn_default_prec_1, width=bar_width, label='NN Default (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 3.5 * bar_width, nn_tuned_prec_1, width=bar_width, label='NN Tuned (Class 1)', alpha=0.6, hatch='//')\n",
        "\n",
        "# Labels and formatting\n",
        "# ax.set_xlabel('Stage')\n",
        "ax.set_ylabel('Precision')\n",
        "ax.set_title('Precision Comparison Across Stages, Models, and Classes')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(stages)\n",
        "ax.set_ylim(0.4, 1.05)\n",
        "ax.legend(ncol=2)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5CEAmgGiXo0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage labels\n",
        "stages = ['Early-stage', 'Mid-stage', 'Late-stage']\n",
        "\n",
        "# Recall values for Class 0\n",
        "xgb_default_recall_0 = [0.7700, 0.7800, 0.9800]\n",
        "xgb_tuned_recall_0   = [0.7796, 0.7800, 0.9900]\n",
        "nn_default_recall_0  = [0.7377, 0.7790, 0.9280]\n",
        "nn_tuned_recall_0    = [0.7337, 0.7137, 0.7137]\n",
        "\n",
        "# Recall values for Class 1\n",
        "xgb_default_recall_1 = [0.8700, 0.8800, 0.9900]\n",
        "xgb_tuned_recall_1   = [0.8700, 0.8800, 0.9800]\n",
        "nn_default_recall_1  = [0.8838, 0.8695, 0.9856]\n",
        "nn_tuned_recall_1    = [0.8911, 0.9174, 0.9174]\n",
        "\n",
        "# Bar width and positions\n",
        "bar_width = 0.1\n",
        "x = np.arange(len(stages))\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Class 0 Recall bars\n",
        "ax.bar(x - 3.5 * bar_width, xgb_default_recall_0, width=bar_width, label='XGB Default (Class 0)')\n",
        "ax.bar(x - 2.5 * bar_width, xgb_tuned_recall_0, width=bar_width, label='XGB Tuned (Class 0)')\n",
        "ax.bar(x - 1.5 * bar_width, nn_default_recall_0, width=bar_width, label='NN Default (Class 0)')\n",
        "ax.bar(x - 0.5 * bar_width, nn_tuned_recall_0, width=bar_width, label='NN Tuned (Class 0)')\n",
        "\n",
        "# Class 1 Recall bars (hatch and transparency)\n",
        "ax.bar(x + 0.5 * bar_width, xgb_default_recall_1, width=bar_width, label='XGB Default (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 1.5 * bar_width, xgb_tuned_recall_1, width=bar_width, label='XGB Tuned (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 2.5 * bar_width, nn_default_recall_1, width=bar_width, label='NN Default (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 3.5 * bar_width, nn_tuned_recall_1, width=bar_width, label='NN Tuned (Class 1)', alpha=0.6, hatch='//')\n",
        "\n",
        "# Labels and formatting\n",
        "# ax.set_xlabel('Stage')\n",
        "ax.set_ylabel('Recall')\n",
        "ax.set_title('Recall Comparison Across Stages, Models, and Classes')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(stages)\n",
        "ax.set_ylim(0.6, 1.05)\n",
        "ax.legend(ncol=2)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H7oJcxNnX-Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stages = ['Early-stage', 'Mid-stage', 'Late-stage']\n",
        "cm_metrics = ['TN', 'FP', 'FN', 'TP']  # Use these keys instead\n",
        "\n",
        "# Confusion matrix data (default and tuned) for each model and stage\n",
        "xgb_default_cm = {\n",
        "    'TN': [578, 587, 705],\n",
        "    'FP': [173, 164, 17],\n",
        "    'FN': [552, 508, 48],\n",
        "    'TP': [3709, 3753, 4201]\n",
        "}\n",
        "\n",
        "xgb_tuned_cm = {\n",
        "    'TN': [578, 585, 712],\n",
        "    'FP': [173, 166, 10],\n",
        "    'FN': [560, 510, 72],\n",
        "    'TP': [3701, 3751, 4177]\n",
        "}\n",
        "\n",
        "nn_default_cm = {\n",
        "    'TN': [554, 585, 670],\n",
        "    'FP': [197, 166, 52],\n",
        "    'FN': [495, 556, 61],\n",
        "    'TP': [3766, 3705, 4188]\n",
        "}\n",
        "\n",
        "nn_tuned_cm = {\n",
        "    'TN': [551, 536, 536],\n",
        "    'FP': [200, 215, 215],\n",
        "    'FN': [464, 352, 352],\n",
        "    'TP': [3797, 3909, 3909]\n",
        "}\n",
        "\n",
        "bar_width = 0.18\n",
        "x = np.arange(len(stages))\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(16, 10), sharex=True)\n",
        "\n",
        "# For prettier titles, map short keys to full names\n",
        "full_names = {\n",
        "    'TN': 'True Negatives (TN)',\n",
        "    'FP': 'False Positives (FP)',\n",
        "    'FN': 'False Negatives (FN)',\n",
        "    'TP': 'True Positives (TP)'\n",
        "}\n",
        "\n",
        "for i, metric in enumerate(cm_metrics):\n",
        "    ax = axs[i // 2, i % 2]\n",
        "\n",
        "    ax.bar(x - 1.5*bar_width, [xgb_default_cm[metric][j] for j in range(3)], width=bar_width, label='XGB Default')\n",
        "    ax.bar(x - 0.5*bar_width, [xgb_tuned_cm[metric][j] for j in range(3)], width=bar_width, label='XGB Tuned')\n",
        "    ax.bar(x + 0.5*bar_width, [nn_default_cm[metric][j] for j in range(3)], width=bar_width, label='NN Default')\n",
        "    ax.bar(x + 1.5*bar_width, [nn_tuned_cm[metric][j] for j in range(3)], width=bar_width, label='NN Tuned')\n",
        "\n",
        "    ax.set_title(full_names[metric])\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(stages)\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    if i % 2 == 0:\n",
        "        ax.set_ylabel('Count')\n",
        "\n",
        "fig.suptitle('Confusion Matrix Components Comparison Across Stages and Models', fontsize=16)\n",
        "fig.legend(loc='upper right', bbox_to_anchor=(0.95, 0.95))\n",
        "plt.tight_layout(rect=[0, 0, 0.9, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WyeooLEgYVPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage labels\n",
        "stages = ['Early-stage', 'Mid-stage', 'Late-stage']\n",
        "\n",
        "# F1-Score values for Class 0\n",
        "xgb_default_f1_0 = [0.6100, 0.6400, 0.9600]\n",
        "xgb_tuned_f1_0   = [0.6100, 0.6300, 0.9500]\n",
        "nn_default_f1_0  = [0.6156, 0.6184, 0.9222]\n",
        "nn_tuned_f1_0    = [0.6240, 0.6541, 0.6541]\n",
        "\n",
        "# F1-Score values for Class 1\n",
        "xgb_default_f1_1 = [0.9100, 0.9200, 0.9900]\n",
        "xgb_tuned_f1_1   = [0.9100, 0.9200, 0.9900]\n",
        "nn_default_f1_1  = [0.9159, 0.9112, 0.9867]\n",
        "nn_tuned_f1_1    = [0.9196, 0.9324, 0.9324]\n",
        "\n",
        "# Bar width and positions\n",
        "bar_width = 0.1\n",
        "x = np.arange(len(stages))\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Class 0 F1-Score bars\n",
        "ax.bar(x - 3.5 * bar_width, xgb_default_f1_0, width=bar_width, label='XGB Default (Class 0)')\n",
        "ax.bar(x - 2.5 * bar_width, xgb_tuned_f1_0, width=bar_width, label='XGB Tuned (Class 0)')\n",
        "ax.bar(x - 1.5 * bar_width, nn_default_f1_0, width=bar_width, label='NN Default (Class 0)')\n",
        "ax.bar(x - 0.5 * bar_width, nn_tuned_f1_0, width=bar_width, label='NN Tuned (Class 0)')\n",
        "\n",
        "# Class 1 F1-Score bars (hatch and transparency)\n",
        "ax.bar(x + 0.5 * bar_width, xgb_default_f1_1, width=bar_width, label='XGB Default (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 1.5 * bar_width, xgb_tuned_f1_1, width=bar_width, label='XGB Tuned (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 2.5 * bar_width, nn_default_f1_1, width=bar_width, label='NN Default (Class 1)', alpha=0.6, hatch='//')\n",
        "ax.bar(x + 3.5 * bar_width, nn_tuned_f1_1, width=bar_width, label='NN Tuned (Class 1)', alpha=0.6, hatch='//')\n",
        "\n",
        "# Labels and formatting\n",
        "ax.set_ylabel('F1-Score')\n",
        "ax.set_title('F1-Score Comparison Across Stages, Models, and Classes')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(stages)\n",
        "ax.set_ylim(0.5, 1.05)\n",
        "ax.legend(ncol=2)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J-5fvpX9ZMS4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
